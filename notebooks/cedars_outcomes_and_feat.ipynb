{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, join(getcwd(), \"../module_code\"))\n",
    "\n",
    "import data.load\n",
    "import cli_utils \n",
    "from data.utils import loading_message\n",
    "\n",
    "sys.argv = [sys.argv[0]]\n",
    "cli_utils.load_cli_args(\"../options.yml\")\n",
    "args = cli_utils.init_cli_args()\n",
    "\n",
    "args.cedars_crrt_data_dir = \"C:/Users/jeffe/OneDrive - UCLA IT Services/UCLA/2023_Winter/Rotation/Data/Cedars\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outcomes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'CRRT Deidentified 2015-2021YTD_VF.xlsx'), sheet_name=\"2015-2021 YTD\")\n",
    "outcomes_ucla = pd.read_excel(os.path.join(args.ucla_crrt_data_dir,'CRRT Deidentified 2015-2021YTD_VF.xlsx'), sheet_name=\"2015-2021 YTD\")\n",
    "\n",
    "# print(outcomes[outcomes.duplicated(subset=['PAT_ID','End Date','CRRT Total Days'])])\n",
    "# ids = outcomes['IP_PATIENT_ID']\n",
    "# print(outcomes[ids.isin(outcomes[outcomes.duplicated(subset=['IP_PATIENT_ID','End Date','CRRT Total Days'])]['IP_PATIENT_ID'])].sort_values(\"IP_PATIENT_ID\"))\n",
    "\n",
    "print(outcomes.columns)\n",
    "print(outcomes_ucla.columns)\n",
    "print(len(outcomes))\n",
    "print(len(outcomes_ucla))\n",
    "\n",
    "outcomes = data.load.load_outcomes(args.cedars_crrt_data_dir, [\"IP_PATIENT_ID\", \"Start Date\"])\n",
    "outcomes_ucla = data.load.load_outcomes(args.ucla_crrt_data_dir, [\"IP_PATIENT_ID\", \"Start Date\"])\n",
    "\n",
    "print(outcomes.columns)\n",
    "print(outcomes_ucla.columns)\n",
    "print(len(outcomes))\n",
    "print(len(outcomes_ucla))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcomes Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granular_outcome = outcomes[[\"Recov. renal funct.\", \"Transitioned to HD\", \"Comfort Care\", \"Expired \"]].idxmax(axis=1)\n",
    "binary_outcome = outcomes[\"recommend_crrt\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "counts = granular_outcome.value_counts()\n",
    "counts = counts.reindex([\"Transitioned to HD\", \"Comfort Care\", \"Expired \", \"Recov. renal funct.\",])\n",
    "\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html\n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    # labels=counts.index,\n",
    "    # autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    # https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "    colors=[\n",
    "        sns.color_palette(\"mako\")[-1],\n",
    "        sns.color_palette(\"rocket\")[-1],\n",
    "        sns.color_palette(\"rocket\")[-3],\n",
    "        sns.color_palette(\"mako\")[-3],\n",
    "    ]\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "counts = binary_outcome.value_counts()\n",
    "# plt.pie(counts,  \n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    labels=[\"Recommend CRRT\", \"Do Not Recommend CRRT\", ],\n",
    "    colors=[\n",
    "            sns.color_palette(\"mako\")[-3],\n",
    "            sns.color_palette(\"rocket\")[-3],\n",
    "    ],\n",
    "    startangle=300\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granular_outcome = outcomes_ucla[[\"Recov. renal funct.\", \"Transitioned to HD\", \"Comfort Care\", \"Expired \"]].idxmax(axis=1)\n",
    "binary_outcome = outcomes_ucla[\"recommend_crrt\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "counts = granular_outcome.value_counts()\n",
    "counts = counts.reindex([\"Transitioned to HD\", \"Comfort Care\", \"Expired \", \"Recov. renal funct.\",])\n",
    "\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.pie.html\n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    # labels=counts.index,\n",
    "    # autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    # https://seaborn.pydata.org/tutorial/color_palettes.html\n",
    "    colors=[\n",
    "        sns.color_palette(\"mako\")[-1],\n",
    "        sns.color_palette(\"rocket\")[-1],\n",
    "        sns.color_palette(\"rocket\")[-3],\n",
    "        sns.color_palette(\"mako\")[-3],\n",
    "    ]\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "counts = binary_outcome.value_counts()\n",
    "# plt.pie(counts,  \n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    labels=[\"Recommend CRRT\", \"Do Not Recommend CRRT\", ],\n",
    "    colors=[\n",
    "            sns.color_palette(\"mako\")[-3],\n",
    "            sns.color_palette(\"rocket\")[-3],\n",
    "    ],\n",
    "    startangle=300\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Total\n",
    "crrt_days = outcomes['CRRT Total Days'].value_counts().sort_index()\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.bar(crrt_days.index, height=crrt_days.values)\n",
    "plt.title('Distribution of CRRT Total Days')\n",
    "plt.xlabel('CRRT Total Days')\n",
    "plt.ylabel('Number of Patients')\n",
    "\n",
    "recommend_days = outcomes[outcomes['recommend_crrt'] == 1]\n",
    "crrt_days = recommend_days['CRRT Total Days'].value_counts().sort_index()\n",
    "plt.bar(crrt_days.index, height=crrt_days.values)\n",
    "plt.legend([\"Not recommend\", \"Recommend\"])\n",
    "plt.show()\n",
    "\n",
    "print(outcomes['CRRT Total Days'].describe())\n",
    "print(recommend_days['CRRT Total Days'].describe())\n",
    "\n",
    "not_recommend_days = outcomes[outcomes['recommend_crrt'] != 1]\n",
    "print(not_recommend_days['CRRT Total Days'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Total\n",
    "crrt_days = outcomes_ucla['CRRT Total Days'].value_counts().sort_index()\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "plt.bar(crrt_days.index, height=crrt_days.values)\n",
    "plt.title('Distribution of CRRT Total Days')\n",
    "plt.xlabel('CRRT Total Days')\n",
    "plt.ylabel('Number of Patients')\n",
    "\n",
    "recommend_days = outcomes_ucla[outcomes_ucla['recommend_crrt'] == 1]\n",
    "crrt_days = recommend_days['CRRT Total Days'].value_counts().sort_index()\n",
    "plt.bar(crrt_days.index, height=crrt_days.values)\n",
    "plt.legend([\"Not recommend\", \"Recommend\"])\n",
    "plt.show()\n",
    "\n",
    "print(outcomes_ucla['CRRT Total Days'].describe())\n",
    "print(recommend_days['CRRT Total Days'].describe())\n",
    "\n",
    "not_recommend_days = outcomes_ucla[outcomes_ucla['recommend_crrt'] != 1]\n",
    "print(not_recommend_days['CRRT Total Days'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique = len(outcomes.index.get_level_values('IP_PATIENT_ID').unique())\n",
    "print('Number of Unique Patients: ', n_unique)\n",
    "\n",
    "repeats_per_patient = outcomes.index.get_level_values('IP_PATIENT_ID').value_counts().values\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "counts, edges, bars = plt.hist(repeats_per_patient, bins=range(min(repeats_per_patient), max(repeats_per_patient) + 2, 1), align='left')\n",
    "plt.xticks(range(min(repeats_per_patient), max(repeats_per_patient) + 2, 1))\n",
    "plt.bar_label(bars)\n",
    "plt.title('Histogram of Patients with Multiple CRRTs')\n",
    "plt.xlabel('Number of Treatments')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.show()\n",
    "\n",
    "# for i in range(min(repeats_per_patient)+1, max(repeats_per_patient)+1):\n",
    "#     n_treatments = outcomes.index.get_level_values('IP_PATIENT_ID').value_counts()\n",
    "#     n_treatments = n_treatments[n_treatments==i].index\n",
    "#     recommend_days = outcomes[outcomes.index.get_level_values('IP_PATIENT_ID').isin(n_treatments)].reset_index()\n",
    "#     recommend_days = recommend_days.set_index(['IP_PATIENT_ID','Num Prev CRRT Treatments'])\n",
    "#     fig = plt.figure(figsize=(10,6))\n",
    "#     recommend_days.unstack('IP_PATIENT_ID')['recommend_crrt'].plot.line()\n",
    "#     plt.legend([])\n",
    "#     plt.show()\n",
    "\n",
    "for i in range(min(repeats_per_patient), max(repeats_per_patient)+1):\n",
    "    print(f'Patients with {i} treatments')\n",
    "    n_treatments = outcomes.index.get_level_values('IP_PATIENT_ID').value_counts()\n",
    "    n_treatments = n_treatments[n_treatments==i].index\n",
    "    \n",
    "    recommend_days = outcomes[outcomes.index.get_level_values('IP_PATIENT_ID').isin(n_treatments)]\n",
    "\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    for j in range(i):\n",
    "        plt.subplot(1, i+1, j+1)\n",
    "        temp = recommend_days[recommend_days['Num Prev CRRT Treatments'] == j]['recommend_crrt']\n",
    "        counts, edges, bars = plt.hist(temp, bins=[0,1,2], align='left')\n",
    "        plt.xticks([0,1])\n",
    "        plt.bar_label(bars)\n",
    "        plt.gca().set_title(f'After Treatment {j+1}')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = outcomes['CRRT Year']\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "counts, edges, bars = plt.hist(years, bins=range(min(years), max(years) + 2, 1), align='left')\n",
    "plt.xticks(range(min(years), max(years) + 1, 1))\n",
    "plt.bar_label(bars)\n",
    "plt.title('Histogram of Treatment Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Patients')\n",
    "\n",
    "recommend_days = outcomes[outcomes['recommend_crrt'] == 1]\n",
    "recommend_years = recommend_days['CRRT Year']\n",
    "counts, edges, bars = plt.hist(recommend_years, bins=range(min(years), max(years) + 2, 1), align='left')\n",
    "plt.bar_label(bars)\n",
    "plt.legend([\"Not recommend\", \"Recommend\"])\n",
    "plt.show()\n",
    "\n",
    "print(f\"min date: {outcomes['End Date'].min()}\")\n",
    "print(f\"max date: {outcomes['End Date'].max()}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCLA to Cedars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_features = [\n",
    "    \"karumanchi_00001867_adt - anonymized.xlsx\", # admission, discharge, transfer\n",
    "    \"karumanchi_00001867_allergy - anonymized.xlsx\",\n",
    "    \"karumanchi_00001867_demographics - anonymized.xlsx\",\n",
    "    \"karumanchi_00001867_enc_dx - anonymized.xlsx\",\n",
    "    \"karumanchi_00001867_encounters - anonymized.xlsx\",\n",
    "    \"karumanchi_00001867_family_hx - anonymized.xlsx\",\n",
    "    \"karumanchi_00001867_flowsheet_daily_io - anonymized.xlsx\", # have to resave as xlsx\n",
    "    \"karumanchi_00001867_flowsheet_vitals - anonymized.xlsx\", # have to resave as xlsx\n",
    "    \"karumanchi_00001867_labs - anonymized.xlsx\", \n",
    "    \"karumanchi_00001867_medications - anonymized.xlsx\",\n",
    "    \"karumanchi_00001867_problem_list - anonymized.xlsx\",\n",
    "    \"karumanchi_00001867_procedures - anonymized.xlsx\", # have to resave as xlsx\n",
    "    \"karumanchi_00001867_social_hx - anonymized (1).xlsx\",\n",
    "]\n",
    "\n",
    "ucla_features = [\n",
    "    'Allergies.txt',\n",
    "    'Encounter_Diagnoses.txt',\n",
    "    'Encounters.txt',\n",
    "    'Family_History.txt',\n",
    "    'Flowsheet_Vitals.txt',\n",
    "    'Hospital_Unit_Transfers.txt',\n",
    "    'Labs.txt',\n",
    "    'Medications.txt',\n",
    "    'Patient_Demographics.txt',\n",
    "    'Problem_List_Diagnoses.txt',\n",
    "    'Problem_Lists.txt',\n",
    "    'Procedures.txt',\n",
    "    'Providers.txt',\n",
    "    'Social_History.txt'\n",
    "]\n",
    "\n",
    "mapping = {\n",
    "    \"karumanchi_00001867_adt - anonymized.xlsx\": 'Hospital_Unit_Transfers.txt',\n",
    "    \"karumanchi_00001867_allergy - anonymized.xlsx\": 'Allergies.txt',\n",
    "    \"karumanchi_00001867_demographics - anonymized.xlsx\": 'Patient_Demographics.txt',\n",
    "    \"karumanchi_00001867_enc_dx - anonymized.xlsx\": 'Encounter_Diagnoses.txt',\n",
    "    \"karumanchi_00001867_encounters - anonymized.xlsx\": 'Encounters.txt',\n",
    "    \"karumanchi_00001867_family_hx - anonymized.xlsx\": 'Family_History.txt',\n",
    "    \"karumanchi_00001867_flowsheet_daily_io - anonymized.xlsx\": None,\n",
    "    \"karumanchi_00001867_flowsheet_vitals - anonymized.xlsx\": 'Flowsheet_Vitals.txt',\n",
    "    \"karumanchi_00001867_labs - anonymized.xlsx\": 'Labs.txt',\n",
    "    \"karumanchi_00001867_medications - anonymized.xlsx\": 'Medications.txt',\n",
    "    \"karumanchi_00001867_problem_list - anonymized.xlsx\": 'Problem_Lists.txt', #'Problem_List_Diagnoses.txt'\n",
    "    \"karumanchi_00001867_procedures - anonymized.xlsx\": 'Procedures.txt',\n",
    "    \"karumanchi_00001867_social_hx - anonymized (1).xlsx\": 'Social_History.txt',\n",
    "} # Missing from cedars: Providers\n",
    "\n",
    "# UCLA ignore\n",
    "ucla_ignore = [\n",
    "    'Patient_Identifiers.txt',\n",
    "]\n",
    "\n",
    "# Used in UCLA\n",
    "FILE_NAMES = {\n",
    "    \"cpt\": \"Procedures.txt\",\n",
    "    \"pr\": \"Problem_Lists.txt\",\n",
    "    \"pr_dx\": \"Problem_List_Diagnoses.txt\",\n",
    "    \"labs\": \"Labs.txt\",\n",
    "    \"rx\": \"Medications.txt\",\n",
    "    \"vitals\": \"Flowsheet_Vitals.txt\",\n",
    "    \"dx\": \"Encounter_Diagnoses.txt\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cedars Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils import read_files_and_combine\n",
    "\n",
    "enc = pd.read_csv(os.path.join(args.cedars_crrt_data_dir, 'Encounters.txt'))\n",
    "\n",
    "for file in mapping.keys():\n",
    "    # df = read_files_and_combine([file], args.cedars_crrt_data_dir, on=[\"IP_PATIENT_ID\"])\n",
    "\n",
    "    if mapping[file] is None:\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(os.path.join(args.cedars_crrt_data_dir, mapping[file]))\n",
    "    # df2 = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, file))\n",
    "    # df2.rename(columns={'PAT_ID': \"IP_PATIENT_ID\", 'PAT_ENC_CSN_ID':'IP_ENCOUNTER_ID'}, inplace=True)\n",
    "    # assert all(x == y for x, y in zip(df.columns, df2.columns)), (df.columns, df2.columns)\n",
    "    # for column in df.columns:\n",
    "    #     if 'Month' in column or 'time' in column.lower() or 'date' in column.lower():\n",
    "    #         continue\n",
    "    #     set1 = set(df[column].unique())\n",
    "    #     set2 = set(df2[column].unique())\n",
    "    #     print(column, set1.difference(set2))\n",
    "\n",
    "    if 'labs' in file:\n",
    "        df = df.merge(enc[['IP_ENCOUNTER_ID', 'IP_PATIENT_ID']], on='IP_ENCOUNTER_ID', how='left')\n",
    "        # coalesce cost column to get first non NA value\n",
    "        df['IP_PATIENT_ID'] = df['IP_PATIENT_ID_x'].combine_first(df['IP_PATIENT_ID_y'])\n",
    "        # remove the cols\n",
    "        df = df.drop(['IP_PATIENT_ID_x', 'IP_PATIENT_ID_y'], 1)\n",
    "        \n",
    "    merged = pd.merge(df, outcomes, on=\"IP_PATIENT_ID\", how=\"inner\")\n",
    "    \n",
    "    print(\"*\"*25 +  f\" {file.split('_')[2]} \" + \"*\" * 25)\n",
    "    print(f\"N null: {df['IP_PATIENT_ID'].isna().sum()}, N entries: {len(df)}, N unique IDs: {len(df.drop_duplicates('IP_PATIENT_ID'))}\")\n",
    "    print(f\"MERGED WITH OUTCOME: N entries {len(merged)}, N unique IDs: {len(merged.drop_duplicates('IP_PATIENT_ID'))}\")\n",
    "    # print(f\"Difference in # unique patients: {len(outcomes)  - len(df.drop_duplicates('IP_PATIENT_ID'))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCLA Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.utils import read_files_and_combine\n",
    "\n",
    "enc = pd.read_csv(os.path.join(args.cedars_crrt_data_dir, 'Encounters.txt'))\n",
    "\n",
    "for file in mapping.keys():\n",
    "    # df = read_files_and_combine([file], args.cedars_crrt_data_dir, on=[\"IP_PATIENT_ID\"])\n",
    "\n",
    "    if mapping[file] is None:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, mapping[file]))\n",
    "    except:\n",
    "        df = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, mapping[file]), encoding='cp1252')\n",
    "    \n",
    "    df.columns = [col.upper() for col in df.columns]\n",
    "\n",
    "    # if 'labs' in file:\n",
    "    #     df = pd.merge(df, enc[['IP_ENCOUNTER_ID', 'IP_PATIENT_ID']], on='IP_ENCOUNTER_ID', how='left')\n",
    "    #     # coalesce cost column to get first non NA value\n",
    "    #     df['IP_PATIENT_ID'] = df['IP_PATIENT_ID_x'].combine_first(df['IP_PATIENT_ID_y'])\n",
    "    #     # remove the cols\n",
    "    #     df = df.drop(['IP_PATIENT_ID_x', 'IP_PATIENT_ID_y'], 1)\n",
    "        \n",
    "    merged = pd.merge(df, outcomes_ucla, on=\"IP_PATIENT_ID\", how=\"inner\")\n",
    "    \n",
    "    print(\"*\"*25 +  f\" {file.split('_')[2]} \" + \"*\" * 25)\n",
    "    print(f\"N null: {df['IP_PATIENT_ID'].isna().sum()}, N entries: {len(df)}, N unique IDs: {len(df.drop_duplicates('IP_PATIENT_ID'))}\")\n",
    "    print(f\"MERGED WITH OUTCOME: N entries {len(merged)}, N unique IDs: {len(merged.drop_duplicates('IP_PATIENT_ID'))}\")\n",
    "    # print(f\"Difference in # unique patients: {len(outcomes)  - len(df.drop_duplicates('IP_PATIENT_ID'))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons of features (i.e., DataFrame Columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospital Transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_adt - anonymized.xlsx'))\n",
    "hospital_transfer = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Hospital_Unit_Transfers.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adt.columns)\n",
    "print(hospital_transfer.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* Same columns under different names\n",
    "* We don't use these futures, just keeping here for reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allergies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_allerg = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_allergy - anonymized.xlsx'))\n",
    "ucla_allerg = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Allergies.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_allerg.columns)\n",
    "print(ucla_allerg.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_demog = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_demographics - anonymized.xlsx'))\n",
    "ucla_demog = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Patient_Demographics.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_demog.columns)\n",
    "print(ucla_demog.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_demog.columns)\n",
    "print(ucla_demog.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_demog_clean = cedars_demog.rename(\n",
    "            {\n",
    "                # CONTROLS\n",
    "                \"GENDER\": \"SEX\",\n",
    "                \"IP_CURRENT_PCP_ID\": \"PCP_IP_PROVIDER_ID\",\n",
    "                \"VITAL_STATUS\": \"KNOWN_DECEASED\",\n",
    "\n",
    "                # CEDARS\n",
    "                'CURRENT_AGE': \"AGE\",\n",
    "                'RACE_1': 'RACE',\n",
    "                'ETHNIC_GROUP': 'ETHNICITY',\n",
    "                'LIVING_STATUS': 'KNOWN_DECEASED',\n",
    "                \"CURRENT_PCP_ID\": \"PCP_IP_PROVIDER_ID\",\n",
    "                'PAT_ID': \"IP_PATIENT_ID\"\n",
    "            },\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "# Cleanup RACE for Cedars data. Combine RACE_2 and RACE_3 into multiple races\n",
    "if 'RACE_2' in cedars_demog_clean.columns and 'RACE_3' in cedars_demog_clean.columns:\n",
    "    cedars_demog_clean.loc[cedars_demog_clean['RACE_2'].notna() | cedars_demog_clean['RACE_3'].notna(), 'RACE'] = 'Multiple Races'\n",
    "    cedars_demog_clean = cedars_demog_clean.replace({'RACE': {'White': 'White or Caucasian', 'Patient Declined': 'Patient Refused'}}) \n",
    "    cedars_demog_clean = cedars_demog_clean.drop([\"RACE_2\",\"RACE_3\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = cedars_demog['CURRENT_AGE']\n",
    "plt.figure(figsize=(20,10))\n",
    "counts, edges, bars = plt.hist(age, bins=range(0, max(age) + 5, 5), align='mid')\n",
    "plt.bar_label(bars)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sex = cedars_demog['SEX']\n",
    "plt.figure(figsize=(5,5))\n",
    "counts = sex.value_counts()\n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    labels=[\"Male\", \"Female\", \"Unknown\" ],\n",
    "    colors=[\n",
    "            sns.color_palette(\"mako\")[-3],\n",
    "            sns.color_palette(\"rocket\")[-3],\n",
    "            sns.color_palette(\"rocket\")[-2],\n",
    "    ],\n",
    "    startangle=300\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "race = cedars_demog['RACE_1']\n",
    "plt.figure(figsize=(7,7))\n",
    "counts = race.value_counts()\n",
    "print(counts)\n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    labels=list(counts.index),\n",
    "    startangle=300\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "race = cedars_demog_clean['RACE']\n",
    "plt.figure(figsize=(7,7))\n",
    "counts = race.value_counts()\n",
    "print(counts)\n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    labels=list(counts.index),\n",
    "    startangle=300\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "race = cedars_demog['RACE_2']\n",
    "plt.figure(figsize=(7,7))\n",
    "counts = race.value_counts()\n",
    "print(counts)\n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    labels=list(counts.index),\n",
    "    startangle=300\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "race = cedars_demog['RACE_3']\n",
    "plt.figure(figsize=(7,7))\n",
    "counts = race.value_counts()\n",
    "print(counts)\n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    labels=list(counts.index),\n",
    "    startangle=300\n",
    ")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ethnicity = cedars_demog_clean['ETHNICITY']\n",
    "plt.figure(figsize=(5,5))\n",
    "counts = ethnicity.value_counts()\n",
    "print(counts)\n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    labels=list(counts.index),\n",
    "    startangle=300\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ethnicity = cedars_demog['LIVING_STATUS']\n",
    "plt.figure(figsize=(5,5))\n",
    "counts = ethnicity.value_counts()\n",
    "print(counts)\n",
    "plt.pie(\n",
    "    counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(counts)/100),\n",
    "    labels=list(counts.index),\n",
    "    startangle=300\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* RACE_3 is basically empty, don't use\n",
    "* RACE_2 is also almost empty - consists of few race descriptions, mostly 'Other' or 'Empty'.\n",
    "* Probaly always take RACE_1. Or have to create a new 'Mixed' column\n",
    "* Some descriptions are different (e.g. Cedars has \"White\" but UCLA has \"White or Caucausian\")\n",
    "* Cedars has \"Alive/Deceased\" but UCLA has \"Known Deceased/Not Known Deceased\"\n",
    "* Age is int not float"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encounters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_enc = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_encounters - anonymized.xlsx'))\n",
    "ucla_enc = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Encounters.txt'), encoding=\"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_enc.columns)\n",
    "print(ucla_enc.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encounters Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_enc_dx = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_enc_dx - anonymized.xlsx'))\n",
    "ucla_enc_dx = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Encounter_Diagnoses.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_enc_dx.columns)\n",
    "print(ucla_enc_dx.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_enc_dx.groupby(\"CURRENT_ICD10_LIST\").size().sort_values(ascending=False)[:15])\n",
    "icd_dx_dates = pd.to_datetime(cedars_enc_dx[\"CONTACT_DATE\"])\n",
    "print(f\"min date: {icd_dx_dates.min()}, max_date: {icd_dx_dates.max()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* UCLA has ICD_TYPE. Cedars it's all ICD10\n",
    "\n",
    "left (cedars), right (ucla)\n",
    "* CONACT_DATE == DIAGNOSIS_DATE\n",
    "* CURRENT_ICD10_LIST == ICD_CODE\n",
    "* DX_NAME == ICD_DESC\n",
    "* PRIMARY_DX_YN == PRIMARY_DIAGNOSIS_FLAG (Y/N ==> S/P)\n",
    "* HSP_FINAL_DIAGNOSIS_FLAG == HSP_FINAL_DIAGNOSIS_FLAG (Y/N ==> 0.0/1.0)\n",
    "* PRESENT_ON_ADM == PRESENT_ON_ADMIT_FLAG (Y/N ==> 0.0/1.0)\n",
    "\n",
    "* Cedars has no 'ADMIT_DIAGNOSIS_FLAG'\n",
    "* Cedars has extra 'RECORD_TYPE'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Family History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_fam = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_family_hx - anonymized.xlsx'))\n",
    "ucla_fam = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Family_History.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_fam.columns)\n",
    "print(ucla_fam.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucla_flow = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Flowsheet_Vitals.txt'))\n",
    "cedars_flow = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_flowsheet_vitals - anonymized.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_flow_daily = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_flowsheet_daily_io - anonymized.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cedars_flow_daily.columns)\n",
    "print(ucla_flow.columns)\n",
    "print(cedars_flow.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* UCLA doesn't have the equivalent 'daily_io'\n",
    "* UCLA has 'VITAL_SIGN_SOURCE' which is all just 'FLOWSHEET'\n",
    "* VITAL_SIGN_TYPE == MEAS_NAME (Cedars - ALL CAPS)\n",
    "* VITAL_SIGN_VALUE == MEAS_VALUE\n",
    "* VITAL_SIGN_TAKEN_TIME == RECORDED_TIME\n",
    "\n",
    "* Cedars BP maybe needs to be separated into SBP and DBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_flow[\"MEAS_NAME\"].replace({\"BP\": \"SBP/DBP\"}, inplace=True)\n",
    "cedars_flow_drop = cedars_flow.dropna(subset=['MEAS_VALUE'])\n",
    "explode_cols = [\"MEAS_VALUE\", \"MEAS_NAME\"]\n",
    "cedars_flow_drop['MEAS_VALUE'] = cedars_flow_drop['MEAS_VALUE'].astype(str)\n",
    "\n",
    "# Ref: https://stackoverflow.com/a/57122617/1888794\n",
    "# don't explode the columsn you set index to, explode the rest via apply, reset everything to normal\n",
    "cedars_flow_splitbp = cedars_flow_drop.apply(\n",
    "        lambda col: col.str.split(\"/\") if col.name in explode_cols else col\n",
    "    ).explode(explode_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_flow_splitbp['MEAS_NAME'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_labs = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_labs - anonymized.xlsx'))\n",
    "ucla_labs = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Labs.txt'),  encoding=\"cp1252\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_labs.columns)\n",
    "print(ucla_labs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ucla_labs['IP_PATIENT_ID'].isnull().values.any())\n",
    "print(cedars_labs['PAT_ID'].isnull().values.any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* Same except Cedars \"NAME\" is UCLA \"COMPONENT_NAME\"\n",
    "* Lots of missing PAT_ID in cedars - use enc_id?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_meds = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_medications - anonymized.xlsx'))\n",
    "ucla_meds = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Medications.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_meds.columns)\n",
    "print(ucla_meds.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "CEDARS ==> UCLA\n",
    "* NAME == MEDICATION_NAME\n",
    "* TAKEN_TIME == TAKEN_TIME_ORDER_DATE\n",
    "* PHARM_CLASS == MEDISPAN_CLASS_NAME\n",
    "* ORDERING_DATE == ORDER_DATE\n",
    "\n",
    "* CEDARS has ROUTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_problems = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_problem_list - anonymized.xlsx'))\n",
    "cedars_problems_updated = pd.read_excel(os.path.join(args.cedars_crrt_data_dir,'karumanchi_00001867_problem_list - anonymized - updated.xlsx'))\n",
    "ucla_problems = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Problem_Lists.txt'),encoding=\"cp1252\")\n",
    "ucla_problems_dx = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Problem_List_Diagnoses.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cedars_problems)\n",
    "display(cedars_problems[cedars_problems['NOTED_DATE'].isna()])\n",
    "display(cedars_problems[~cedars_problems['DATE_OF_ENTRY'].isna()]['STATUS'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ucla_problems)\n",
    "display(ucla_problems[ucla_problems['noted_date'].isna()])\n",
    "display(ucla_problems[ucla_problems['date_of_entry'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cedars_problems_updated)\n",
    "display(cedars_problems_updated[cedars_problems_updated['NOTED_DATE'].isna()])\n",
    "display(cedars_problems_updated[~cedars_problems_updated['DATE_OF_ENTRY'].isna()]['STATUS'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_problems.columns)\n",
    "print(cedars_problems_updated.columns)\n",
    "print(ucla_problems.columns)\n",
    "print(ucla_problems_dx.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hcuppy.ccs import CCSEngine\n",
    "ce = CCSEngine(mode=\"dx\")\n",
    "ccs_dict = cedars_problems[cedars_problems['STATUS']=='ACTIVE']['CURRENT_ICD10_LIST'].apply(lambda icd_code: ce.get_ccs(icd_code))\n",
    "# series of dicts, explode each dict attribute to its own column\n",
    "ccs_dict = pd.DataFrame(ccs_dict.values.tolist())\n",
    "ccs_dict.columns = [\"CCS_CODE\", \"CCS_DESCRIPTION\", \"CCS_LEVEL1\", \"CCS_LEVEL1_DESCRIPTION\", \"CCS_LEVEL2\", \"CCS_LEVEL2_DESCRIPTION\"]\n",
    "\n",
    "# combine the granular icd codes with the higher level CCS ones\n",
    "cedars_problems_ccs = pd.concat([cedars_problems, ccs_dict], axis=1)\n",
    "\n",
    "print(cedars_problems_ccs.groupby(\"CCS_CODE\").size().sort_values(ascending=False)[:15])\n",
    "print(len(cedars_problems_ccs['CCS_CODE'].unique()))\n",
    "onehot_ccs = pd.get_dummies(cedars_problems_ccs[[\"PAT_ID\", \"CCS_CODE\"]], columns=[\"CCS_CODE\"])\n",
    "onehot_ccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hcuppy.ccs import CCSEngine\n",
    "ce = CCSEngine(mode=\"dx\")\n",
    "ccs_dict = cedars_problems_updated[cedars_problems_updated['STATUS']=='ACTIVE']['CURRENT_ICD10_LIST'].apply(lambda icd_code: ce.get_ccs(icd_code))\n",
    "# series of dicts, explode each dict attribute to its own column\n",
    "ccs_dict = pd.DataFrame(ccs_dict.values.tolist())\n",
    "ccs_dict.columns = [\"CCS_CODE\", \"CCS_DESCRIPTION\", \"CCS_LEVEL1\", \"CCS_LEVEL1_DESCRIPTION\", \"CCS_LEVEL2\", \"CCS_LEVEL2_DESCRIPTION\"]\n",
    "\n",
    "# combine the granular icd codes with the higher level CCS ones\n",
    "cedars_problems_ccs = pd.concat([cedars_problems_updated, ccs_dict], axis=1)\n",
    "\n",
    "print(cedars_problems_ccs.groupby(\"CCS_CODE\").size().sort_values(ascending=False)[:15])\n",
    "print(len(cedars_problems_ccs['CCS_CODE'].unique()))\n",
    "\n",
    "onehot_ccs = pd.get_dummies(cedars_problems_ccs[[\"PAT_ID\", \"CCS_CODE\"]], columns=[\"CCS_CODE\"])\n",
    "onehot_ccs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "* CEDARS problems combines problem list and problem list dx\n",
    "\n",
    "\n",
    "* CONTACT_DATE => encounter_date\n",
    "* DESCRIPTION => prob_desc\n",
    "* STATUS => problem_status\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* UCLA dx has 'diagnosis source; which is just 'PROBLEM_LIST'\n",
    "* UCLA has ICD_TYPE. Cedars it's all ICD10\n",
    "* DX_NAME => icd_desc\n",
    "* Cedars can have multiple codes??? \n",
    "* Cedars has ACCT_DC_YN\n",
    "* Cedars has PRIOTITY\n",
    "* UCLA has diagnosis_date\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_proc = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_procedures - anonymized.xlsx'))\n",
    "ucla_proc = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Procedures.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_proc.columns)\n",
    "print(ucla_proc.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* PROC_START_TIME is more granular than PROC_DATE\n",
    "* Weird proc codes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_soc = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_social_hx - anonymized (1).xlsx'))\n",
    "ucla_soc = pd.read_csv(os.path.join(args.ucla_crrt_data_dir, 'Social_History.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_soc.columns)\n",
    "print(ucla_soc.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* SMOKING_TOBACCO_USE => SMOKING_TOB_STATUS\n",
    "* TOBACCO_USE => TOBACCO_USER\n",
    "\n",
    "* UCLA has CIGARETTES_YN\n",
    "\n",
    "* ALCOHOL_USE => ALCOHOL_USER\n",
    "* ALCOHOL_COMMENT => ALCOHOL_TYPE (MIGHT NEED FORMATTING)\n",
    "\n",
    "* Cedars has ILLICIT_DRUG_USER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Load Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.load\n",
    "import data.preprocess\n",
    "\n",
    "static_df = data.load.load_static_features(args.cedars_crrt_data_dir).set_index(\"IP_PATIENT_ID\")\n",
    "\n",
    "# ids = static_df['PAT_ID']\n",
    "# dupe = static_df[ids.isin(ids[ids.duplicated()])].sort_values(\"PAT_ID\")\n",
    "# print(dupe)\n",
    "\n",
    "temp_df = outcomes.reset_index().set_index('IP_PATIENT_ID')\n",
    "features_with_outcomes = static_df.join(temp_df, how=\"inner\")\n",
    "\n",
    "display(static_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longitudinal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.longitudinal_utils import (\n",
    "    get_time_window_mask\n",
    ")\n",
    "\n",
    "import data.longitudinal_features as lf\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "temp = outcomes.copy()\n",
    "temp.index.rename({'PAT_ID':'IP_PATIENT_ID'}, inplace=True)\n",
    "window = get_time_window_mask(temp, {'YEARS': 10, 'MONTHS': 0, 'DAYS': 7}, None, 'End Date', 0)\n",
    "ucla_window = get_time_window_mask(outcomes_ucla.copy(), {'YEARS': 10, 'MONTHS': 0, 'DAYS': 7}, None, 'End Date', 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses = lf.load_diagnoses(args.cedars_crrt_data_dir,\n",
    "                        time_window = window, time_interval='1D')\n",
    "display(diagnoses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals = lf.load_vitals(args.cedars_crrt_data_dir,\n",
    "                        time_window = window, time_interval='1D')\n",
    "display(vitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(vitals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds = lf.load_medications(args.cedars_crrt_data_dir, \n",
    "                        time_window = window, time_interval=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(meds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = lf.load_labs(args.cedars_crrt_data_dir, \n",
    "                        time_window = window, time_interval=None)\n",
    "display(labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(labs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = lf.load_problems(args.cedars_crrt_data_dir,  \n",
    "                        time_window = window, time_interval=None)\n",
    "display(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(problems)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = lf.load_procedures(args.cedars_crrt_data_dir, \n",
    "                        time_window = window, time_interval=None)\n",
    "display(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(proc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "import itertools\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def jelly_pairwise_diff(left:list, right:list) -> pd.DataFrame:\n",
    "    results = {\n",
    "        combo: jellyfish.levenshtein_distance(*combo) for combo in itertools.product(left, right)\n",
    "    }\n",
    "    distances = pd.DataFrame({'Distance':results.values()}, index = results.keys()).sort_values('Distance')\n",
    "    return distances\n",
    "\n",
    "def fuzzy_pairwise_diff(left:list, right:list, mode:str = None) -> pd.DataFrame:\n",
    "\n",
    "    if mode == 'sort':\n",
    "        comparison_function = fuzz.token_sort_ratio\n",
    "    elif mode == 'partial':\n",
    "        comparison_function = fuzz.partial_ratio\n",
    "    else:\n",
    "        comparison_function = fuzz.ratio\n",
    "    \n",
    "    results = {\n",
    "        combo: comparison_function(*combo) for combo in itertools.product(left, right)\n",
    "    }\n",
    "    distances = pd.DataFrame({'Distance':results.values()}, index = results.keys()).sort_values('Distance', ascending=False)\n",
    "    return distances\n",
    "\n",
    "def assess_value_alignment(df1, df2, cols=None, len_only=False, permute=False):\n",
    "\n",
    "    if cols is None:\n",
    "        cols = set(df1.columns).intersection(set(df2.columns))\n",
    "    else:\n",
    "        cols = set(cols)\n",
    "\n",
    "    cols = cols - set({'IP_PATIENT_ID', 'IP_ENCOUNTER_ID'})\n",
    "    \n",
    "    if not permute:\n",
    "        for column in cols:\n",
    "            unique1 = set(df1[column].unique())\n",
    "            unique2 = set(df2[column].unique())\n",
    "\n",
    "            print('Assessing column: ', column)\n",
    "            if len_only:\n",
    "                print(\"Only in first df: \", len(unique1.difference(unique2)))\n",
    "                print(\"Only in second df: \", len(unique2.difference(unique1)))\n",
    "                print(\"In both df: \", len(unique2.intersection(unique1)))\n",
    "            else:\n",
    "                print(\"Only in first df: \", unique1.difference(unique2))\n",
    "                print(\"Only in second df: \", unique2.difference(unique1))\n",
    "                print(\"In both df: \", unique2.intersection(unique1))\n",
    "            print('')\n",
    "    else:\n",
    "        for column1 in cols:\n",
    "            for column2 in cols:\n",
    "                unique1 = set(df1[column1].unique())\n",
    "                unique2 = set(df2[column2].unique())\n",
    "\n",
    "                print('Assessing column: ', column1, ' vs. ', column2)\n",
    "                if len_only:\n",
    "                    print(\"Only in first df: \", len(unique1.difference(unique2)))\n",
    "                    print(\"Only in second df: \", len(unique2.difference(unique1)))\n",
    "                    print(\"In both df: \", len(unique2.intersection(unique1)))\n",
    "                else:\n",
    "                    print(\"Only in first df: \", unique1.difference(unique2))\n",
    "                    print(\"Only in second df: \", unique2.difference(unique1))\n",
    "                    print(\"In both df: \", unique2.intersection(unique1))\n",
    "                print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.load\n",
    "import numpy as np\n",
    "from pandas import to_numeric\n",
    "\n",
    "cedars_static_df = data.load.read_files_and_combine([\"Patient_Demographics.txt\"], args.cedars_crrt_data_dir, how=\"outer\")\n",
    "ucla_static_df = data.load.read_files_and_combine([\"Patient_Demographics.txt\"], args.ucla_crrt_data_dir, how=\"outer\")\n",
    "\n",
    "def static_preprocess(static_df):\n",
    "    collapse_ethnicity_map = {  # Collapse everything to (Not) Hisp/Lat\n",
    "            \"Not Hispanic or Latino\": [\n",
    "                # UCLA\n",
    "                \"Choose Not to Answer\",\n",
    "                \"Patient Refused\",\n",
    "                \"Unknown\",\n",
    "                # Cedars\n",
    "                \"Non-Hispanic\",\n",
    "                \"Patient Declined\",\n",
    "                np.nan\n",
    "            ],\n",
    "            \"Hispanic or Latino\": [\n",
    "                # UCLA\n",
    "                \"Mexican, Mexican American, Chicano/a\",\n",
    "                \"Hispanic/Spanish origin Other\",\n",
    "                \"Puerto Rican\",\n",
    "                \"Cuban\",\n",
    "                # Cedars\n",
    "                \"Hispanic\",\n",
    "            ],\n",
    "        }\n",
    "    \n",
    "    \n",
    "    column_alignment = {\n",
    "            # CONTROLS\n",
    "            \"GENDER\": \"SEX\",\n",
    "            \"IP_CURRENT_PCP_ID\": \"PCP_IP_PROVIDER_ID\",\n",
    "            \"VITAL_STATUS\": \"KNOWN_DECEASED\",\n",
    "            # CEDARS\n",
    "            \"CURRENT_AGE\": \"AGE\",\n",
    "            \"RACE_1\": \"RACE\",\n",
    "            \"ETHNIC_GROUP\": \"ETHNICITY\",\n",
    "            \"LIVING_STATUS\": \"KNOWN_DECEASED\",\n",
    "            \"CURRENT_PCP_ID\": \"PCP_IP_PROVIDER_ID\",\n",
    "        }\n",
    "    \n",
    "    static_df = static_df.rename(column_alignment, axis=1).replace(\n",
    "            {\n",
    "                \"ETHNICITY\": {\n",
    "                    col: collapsed_name\n",
    "                    for collapsed_name, cols in collapse_ethnicity_map.items()\n",
    "                    for col in cols\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "    #### ADD TO LOAD.PY ####\n",
    "    # Cleanup RACE for Cedars data. Combine RACE_2 and RACE_3 into multiple races\n",
    "    if \"RACE_2\" in static_df.columns and \"RACE_3\" in static_df.columns:\n",
    "        static_df.loc[\n",
    "            static_df[\"RACE_2\"].notna() | static_df[\"RACE_3\"].notna(), \"RACE\"\n",
    "        ] = \"Multiple Races\"\n",
    "        static_df = static_df.replace(\n",
    "            {\n",
    "                \"RACE\": {\n",
    "                    \"White\": \"White or Caucasian\",\n",
    "                    \"Patient Declined\": \"Patient Refused\",\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        static_df = static_df.drop([\"RACE_2\", \"RACE_3\"], axis=1)\n",
    "\n",
    "    static_df[\"AGE\"] = static_df[\"AGE\"].astype(float)\n",
    "    static_df = static_df[\n",
    "        ~((static_df[\"SEX\"] != \"Male\") & (static_df[\"SEX\"] != \"Female\"))\n",
    "    ]\n",
    "\n",
    "    collapse_race_map = {\n",
    "        \"Unknown\": [\n",
    "            \"Other\",\n",
    "            'Patient Refused',\n",
    "            np.nan\n",
    "        ]\n",
    "\n",
    "    }\n",
    "    static_df = static_df.replace(\n",
    "        {\n",
    "                \"RACE\": {\n",
    "                    col: collapsed_name\n",
    "                    for collapsed_name, cols in collapse_race_map.items()\n",
    "                    for col in cols\n",
    "                }\n",
    "            }\n",
    "    )\n",
    "    # static_df = static_df.dropna(subset=[\"AGE\",\"RACE\"])\n",
    "    #### ADD TO LOAD.PY ####\n",
    "\n",
    "    static_df = static_df.drop('PCP_IP_PROVIDER_ID', axis=1, errors=\"ignore\")\n",
    "    # static_df = map_provider_id_to_type(static_df, raw_data_dir)\n",
    "\n",
    "    # explicitly mapping here instead of numerical encoding automatically so that you know which is which when referencing outputs/data/etc.\n",
    "    bin_cols_mapping = {\n",
    "            \"SEX\": {\"Male\": 0, \"Female\": 1},\n",
    "            \"KNOWN_DECEASED\": {\n",
    "                \"Not Known Deceased\": 0,\n",
    "                \"Known Deceased\": 1,\n",
    "                \"Alive\": 0,  # Cedars\n",
    "                \"Deceased\": 1,  # Cedars\n",
    "            },\n",
    "            \"ETHNICITY\": {\n",
    "                \"Not Hispanic or Latino\": 0,\n",
    "                \"Hispanic or Latino\": 1,\n",
    "            },\n",
    "        }\n",
    "    static_df = static_df.replace(bin_cols_mapping)\n",
    "        \n",
    "    return static_df\n",
    "\n",
    "cedars_static_df = static_preprocess(cedars_static_df)\n",
    "ucla_static_df = static_preprocess(ucla_static_df)\n",
    "\n",
    "assess_value_alignment(cedars_static_df, ucla_static_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_static_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* Collased Ethnicity to binary categories (hispanic/non-hispanic)\n",
    "* Renaming of columns to match UCLA\n",
    "* Cast AGE as float\n",
    "* Removal of one \"unknown\" from gender - keep this as unknown\n",
    "* Combine Race_1, Race_2, Race_3 into \"multiple races\" if Race_2 or Race_3 are not nan, otherwise take Race_1\n",
    "* *Drop rows with null ages or race?* - consider null patients as unkown and combine them all with other, unkown, refused, etc. do not remove null age, this will be filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_cedars = cedars_static_df['AGE']\n",
    "plt.figure(figsize=(15,10))\n",
    "counts, edges, bars = plt.hist(age_cedars, bins=range(0, int(max(age_cedars)) + 5, 5), align='mid', alpha=0.5,)\n",
    "plt.bar_label(bars)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "# plt.show()\n",
    "\n",
    "age = ucla_static_df['AGE']\n",
    "# plt.figure(figsize=(20,10))\n",
    "counts, edges, bars = plt.hist(age, bins=range(0, int(max(age_cedars)) + 5, 5), align='mid', alpha=0.5,)\n",
    "plt.bar_label(bars)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Cedars','UCLA'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "def pie_plot(value_counts, title=\"Cedars\", labels=None):\n",
    "    if labels is None:\n",
    "        labels = list(value_counts.index)\n",
    "    plt.pie(\n",
    "    value_counts,\n",
    "    # autopct=\"%1.1f%%\",\n",
    "    autopct=lambda p : '{:.2f}%  ({:,.0f})'.format(p,p * sum(value_counts)/100),\n",
    "    labels=labels,\n",
    "    startangle=300\n",
    "    )\n",
    "    plt.title(title)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "pie_plot(cedars_static_df['SEX'].value_counts(), title=\"Cedars\", labels=['Male', 'Female'])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "pie_plot(ucla_static_df['SEX'].value_counts(), title=\"UCLA\", labels=['Male', 'Female'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "pie_plot(cedars_static_df['RACE'].value_counts().reindex(ucla_static_df['RACE'].value_counts().index), title=\"Cedars\")\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "pie_plot(ucla_static_df['RACE'].value_counts(), title=\"UCLA\")\n",
    "\n",
    "print('-------------------Cedars------------------')\n",
    "print(cedars_static_df['RACE'].value_counts().reindex(ucla_static_df['RACE'].value_counts().index))\n",
    "print('-------------------UCLA------------------')\n",
    "print(ucla_static_df['RACE'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_plot(value_count):\n",
    "    legend = list(value_count.index)\n",
    "    number = value_count.values\n",
    "\n",
    "    bottom = 0\n",
    "    for i in range(len(number)):\n",
    "        plt.bar('Cedars', number[i], bottom=bottom)\n",
    "        bottom = number[i] + bottom\n",
    "        legend[i] = f'{legend[i]}: {number[i]*100:.3f}%'\n",
    "        # plt.text(0.2, bottom-number[i]/2, number[i])\n",
    "    plt.legend(legend)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "stacked_plot(cedars_static_df['RACE'].value_counts(normalize=True))\n",
    "plt.subplot(1,2,2)\n",
    "stacked_plot(ucla_static_df['RACE'].value_counts(normalize=True))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use side-by-side bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "pie_plot(cedars_static_df['ETHNICITY'].value_counts(), title=\"Cedars\", labels=['Non Hispanic or Latino','Hispanic or Latino'])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "pie_plot(ucla_static_df['ETHNICITY'].value_counts(), title=\"UCLA\", labels=['Non Hispanic or Latino','Hispanic or Latino'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "pie_plot(cedars_static_df['KNOWN_DECEASED'].value_counts(), title=\"Cedars\", labels=['Not Known Deceased','Known Deceased'])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "pie_plot(ucla_static_df['KNOWN_DECEASED'].value_counts(), title=\"UCLA\", labels=['Not Known Deceased','Known Deceased'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_dx_df = data.load.read_files_and_combine(['Encounter_Diagnoses.txt'], args.cedars_crrt_data_dir)\n",
    "ucla_dx_df = data.load.read_files_and_combine(['Encounter_Diagnoses.txt'], args.ucla_crrt_data_dir)\n",
    "\n",
    "# Cedars alignment. Assume contact date is diagnosis date since that's the date we have\n",
    "cedars_dx_df = cedars_dx_df.rename({\"CURRENT_ICD10_LIST\": \"ICD_CODE\",\n",
    "                        \"CONTACT_DATE\": \"DIAGNOSIS_DATE\"}, axis=1)\n",
    "\n",
    "assess_value_alignment(cedars_dx_df, ucla_dx_df, cols=['ICD_CODE'], len_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_diag_icd10 = set(cedars_dx_df['ICD_CODE'])\n",
    "ucla_diag_icd10 = set(ucla_dx_df['ICD_CODE'])\n",
    "\n",
    "only_cedars = cedars_diag_icd10.difference(ucla_diag_icd10)\n",
    "only_ucla = ucla_diag_icd10.difference(cedars_diag_icd10)\n",
    "both_diag = cedars_diag_icd10.intersection(ucla_diag_icd10)\n",
    "\n",
    "print('Rows in Cedars Diagnosis File: ', len(cedars_dx_df))\n",
    "print('Rows in Cedars Diagnosis File with codes unique from UCLA: ', \n",
    "      len(cedars_dx_df[cedars_dx_df['ICD_CODE'].isin(only_cedars)]), '|',\n",
    "      len(cedars_dx_df[cedars_dx_df['ICD_CODE'].isin(only_cedars)])/len(cedars_dx_df))\n",
    "print('Rows in Cedars Diagnosis File with codes shared with UCLA: ', \n",
    "      len(cedars_dx_df[cedars_dx_df['ICD_CODE'].isin(both_diag)]),'|',\n",
    "      len(cedars_dx_df[cedars_dx_df['ICD_CODE'].isin(both_diag)])/len(cedars_dx_df))\n",
    "\n",
    "print('Rows in UCLA Diagnosis File: ', len(ucla_dx_df))\n",
    "print('Rows in UCLA Diagnosis File with codes unique from Cedars: ', \n",
    "      len(ucla_dx_df[ucla_dx_df['ICD_CODE'].isin(only_ucla)]),'|',\n",
    "      len(ucla_dx_df[ucla_dx_df['ICD_CODE'].isin(only_ucla)])/len(ucla_dx_df))\n",
    "print('Rows in UCLA Diagnosis File with codes shared with Cedars: ', \n",
    "      len(ucla_dx_df[ucla_dx_df['ICD_CODE'].isin(both_diag)]),'|',\n",
    "      len(ucla_dx_df[ucla_dx_df['ICD_CODE'].isin(both_diag)])/len(ucla_dx_df))\n",
    "\n",
    "# print(cedars_dx_df[cedars_dx_df['ICD_CODE'].isin(only_cedars)]['ICD_CODE'].value_counts()[:10])\n",
    "# print(cedars_dx_df[cedars_dx_df['ICD_CODE'].isin(both_diag)]['ICD_CODE'].value_counts()[:10])\n",
    "\n",
    "# print(ucla_dx_df[ucla_dx_df['ICD_CODE'].isin(only_ucla)]['ICD_CODE'].value_counts()[:10])\n",
    "# print(ucla_dx_df[ucla_dx_df['ICD_CODE'].isin(both_diag)]['ICD_CODE'].value_counts()[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* This one just combine via outer join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_prob = data.load.read_files_and_combine(['Problem_Lists_Outdated.txt'], args.cedars_crrt_data_dir)\n",
    "ucla_prob = data.load.read_files_and_combine(['Problem_Lists.txt', \"Problem_List_Diagnoses.txt\"], args.ucla_crrt_data_dir)\n",
    "\n",
    "print(len(cedars_prob['IP_PATIENT_ID'].unique()))\n",
    "cedars_prob = cedars_prob.rename({'STATUS': 'PROBLEM_STATUS',\n",
    "                                    \"CURRENT_ICD10_LIST\": \"ICD_CODE\"}, axis=1)\n",
    "assess_value_alignment(cedars_prob, ucla_prob, cols=['ICD_CODE'], len_only=True)\n",
    "#### This one just combine via outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_diag_icd10 = set(cedars_prob['ICD_CODE'])\n",
    "ucla_diag_icd10 = set(ucla_prob['ICD_CODE'])\n",
    "\n",
    "only_cedars = cedars_diag_icd10.difference(ucla_diag_icd10)\n",
    "only_ucla = ucla_diag_icd10.difference(cedars_diag_icd10)\n",
    "both_diag = cedars_diag_icd10.intersection(ucla_diag_icd10)\n",
    "\n",
    "print('Rows in Cedars Diagnosis File: ', len(cedars_prob))\n",
    "print('Rows in Cedars Diagnosis File with codes unique from UCLA: ', \n",
    "      len(cedars_prob[cedars_prob['ICD_CODE'].isin(only_cedars)]), '|',\n",
    "      len(cedars_prob[cedars_prob['ICD_CODE'].isin(only_cedars)])/len(cedars_prob))\n",
    "print('Rows in Cedars Diagnosis File with codes shared with UCLA: ', \n",
    "      len(cedars_prob[cedars_prob['ICD_CODE'].isin(both_diag)]),'|',\n",
    "      len(cedars_prob[cedars_prob['ICD_CODE'].isin(both_diag)])/len(cedars_prob))\n",
    "\n",
    "print('Rows in UCLA Diagnosis File: ', len(ucla_prob))\n",
    "print('Rows in UCLA Diagnosis File with codes unique from Cedars: ', \n",
    "      len(ucla_prob[ucla_prob['ICD_CODE'].isin(only_ucla)]),'|',\n",
    "      len(ucla_prob[ucla_prob['ICD_CODE'].isin(only_ucla)])/len(ucla_prob))\n",
    "print('Rows in UCLA Diagnosis File with codes shared with Cedars: ', \n",
    "      len(ucla_prob[ucla_prob['ICD_CODE'].isin(both_diag)]),'|',\n",
    "      len(ucla_prob[ucla_prob['ICD_CODE'].isin(both_diag)])/len(ucla_prob))\n",
    "\n",
    "# print(cedars_prob[cedars_prob['ICD_CODE'].isin(only_cedars)]['ICD_CODE'].value_counts()[:10])\n",
    "# print(cedars_prob[cedars_prob['ICD_CODE'].isin(both_diag)]['ICD_CODE'].value_counts()[:10])\n",
    "\n",
    "# print(ucla_prob[ucla_prob['ICD_CODE'].isin(only_ucla)]['ICD_CODE'].value_counts()[:10])\n",
    "# print(ucla_prob[ucla_prob['ICD_CODE'].isin(both_diag)]['ICD_CODE'].value_counts()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_prob = data.load.read_files_and_combine(['Problem_Lists.txt'], args.cedars_crrt_data_dir)\n",
    "ucla_prob = data.load.read_files_and_combine(['Problem_Lists.txt', \"Problem_List_Diagnoses.txt\"], args.ucla_crrt_data_dir)\n",
    "\n",
    "cedars_prob = cedars_prob.rename({'STATUS': 'PROBLEM_STATUS',\n",
    "                                    \"CURRENT_ICD10_LIST\": \"ICD_CODE\"}, axis=1)\n",
    "\n",
    "print(len(cedars_prob['IP_PATIENT_ID'].unique()))\n",
    "assess_value_alignment(cedars_prob, ucla_prob, cols=['ICD_CODE'], len_only=True)\n",
    "#### This one just combine via outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_diag_icd10 = set(cedars_prob['ICD_CODE'])\n",
    "ucla_diag_icd10 = set(ucla_prob['ICD_CODE'])\n",
    "\n",
    "only_cedars = cedars_diag_icd10.difference(ucla_diag_icd10)\n",
    "only_ucla = ucla_diag_icd10.difference(cedars_diag_icd10)\n",
    "both_diag = cedars_diag_icd10.intersection(ucla_diag_icd10)\n",
    "\n",
    "print('Rows in Cedars Diagnosis File: ', len(cedars_prob))\n",
    "print('Rows in Cedars Diagnosis File with codes unique from UCLA: ', \n",
    "      len(cedars_prob[cedars_prob['ICD_CODE'].isin(only_cedars)]), '|',\n",
    "      len(cedars_prob[cedars_prob['ICD_CODE'].isin(only_cedars)])/len(cedars_prob))\n",
    "print('Rows in Cedars Diagnosis File with codes shared with UCLA: ', \n",
    "      len(cedars_prob[cedars_prob['ICD_CODE'].isin(both_diag)]),'|',\n",
    "      len(cedars_prob[cedars_prob['ICD_CODE'].isin(both_diag)])/len(cedars_prob))\n",
    "\n",
    "print('Rows in UCLA Diagnosis File: ', len(ucla_prob))\n",
    "print('Rows in UCLA Diagnosis File with codes unique from Cedars: ', \n",
    "      len(ucla_prob[ucla_prob['ICD_CODE'].isin(only_ucla)]),'|',\n",
    "      len(ucla_prob[ucla_prob['ICD_CODE'].isin(only_ucla)])/len(ucla_prob))\n",
    "print('Rows in UCLA Diagnosis File with codes shared with Cedars: ', \n",
    "      len(ucla_prob[ucla_prob['ICD_CODE'].isin(both_diag)]),'|',\n",
    "      len(ucla_prob[ucla_prob['ICD_CODE'].isin(both_diag)])/len(ucla_prob))\n",
    "\n",
    "# print(cedars_prob[cedars_prob['ICD_CODE'].isin(only_cedars)]['ICD_CODE'].value_counts()[:10])\n",
    "# print(cedars_prob[cedars_prob['ICD_CODE'].isin(both_diag)]['ICD_CODE'].value_counts()[:10])\n",
    "\n",
    "# print(ucla_prob[ucla_prob['ICD_CODE'].isin(only_ucla)]['ICD_CODE'].value_counts()[:10])\n",
    "# print(ucla_prob[ucla_prob['ICD_CODE'].isin(both_diag)]['ICD_CODE'].value_counts()[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* This one just combine via outer join"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_vitals = data.load.read_files_and_combine(['Flowsheet_Vitals.txt'], args.cedars_crrt_data_dir)\n",
    "ucla_vitals = data.load.read_files_and_combine(['Flowsheet_Vitals.txt'], args.ucla_crrt_data_dir)\n",
    "\n",
    "mapping = {\n",
    "        \"Temp\": \"Temperature\",\n",
    "        \"BMI (Calculated)\": \"BMI\",\n",
    "        \"R BMI\": \"BMI\",\n",
    "        \"WEIGHT/SCALE\": \"Weight\",\n",
    "        \"BP\": \"SBP/DBP\",\n",
    "        \"BLOOD PRESSURE\": \"SBP/DBP\",\n",
    "        \"Resp\": \"Respirations\",\n",
    "        \"PULSE OXIMETRY\": \"SpO2\",\n",
    "\n",
    "        'HEIGHT_IN': 'Height', # inches is correct\n",
    "        'TEMP': 'Temperature',                  \n",
    "        'O2_SATURATION': 'SpO2',\n",
    "        'RESP_RATE': 'Respirations',\n",
    "        'HEART_RATE': 'Pulse',\n",
    "        \"WEIGHT_OZ\": 'Weight' # oz is correct\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## UCLA alignment\n",
    "ucla_vitals = ucla_vitals.replace({\"VITAL_SIGN_TYPE\": mapping})\n",
    "print(len(ucla_vitals))\n",
    "\n",
    "ucla_vitals = ucla_vitals.drop_duplicates(\n",
    "        subset=[\"IP_PATIENT_ID\", \"VITAL_SIGN_TYPE\", \"VITAL_SIGN_TAKEN_TIME\"]\n",
    "    )\n",
    "print(len(ucla_vitals))\n",
    "\n",
    "ucla_vitals = ucla_vitals.apply(\n",
    "        lambda col: col.str.split(\"/\") if col.name in [\"VITAL_SIGN_VALUE\", \"VITAL_SIGN_TYPE\"] else col\n",
    "    ).explode([\"VITAL_SIGN_VALUE\", \"VITAL_SIGN_TYPE\"])\n",
    "\n",
    "print(len(ucla_vitals))\n",
    "\n",
    "ignore_vitals = [\"O2 Device\"]\n",
    "ignore_mask = ~ucla_vitals[\"VITAL_SIGN_TYPE\"].isin(ignore_vitals)\n",
    "ucla_vitals = ucla_vitals[ignore_mask]\n",
    "print(len(ucla_vitals))\n",
    "\n",
    "ucla_vitals[\"VITAL_SIGN_VALUE\"] = ucla_vitals[\"VITAL_SIGN_VALUE\"].astype(float)\n",
    "\n",
    "########## Add to load_vitals\n",
    "cedars_vitals = cedars_vitals.rename({\"MEAS_NAME\": \"VITAL_SIGN_TYPE\",\n",
    "                        \"RECORDED_TIME\": \"VITAL_SIGN_TAKEN_TIME\",\n",
    "                        \"MEAS_VALUE\": \"VITAL_SIGN_VALUE\"}, axis=1)\n",
    "\n",
    "cedars_vitals = cedars_vitals.replace({'VITAL_SIGN_TYPE': mapping}) \n",
    "\n",
    "cedars_vitals = cedars_vitals.dropna(subset=['VITAL_SIGN_VALUE'])\n",
    "print(len(ucla_vitals))\n",
    "\n",
    "cedars_vitals = cedars_vitals.apply(\n",
    "        lambda col: col.str.split(\"/\") if col.name in [\"VITAL_SIGN_VALUE\", \"VITAL_SIGN_TYPE\"] else col\n",
    "    ).explode([\"VITAL_SIGN_VALUE\", \"VITAL_SIGN_TYPE\"])\n",
    "\n",
    "cedars_vitals = cedars_vitals.drop_duplicates(\n",
    "        subset=[\"IP_PATIENT_ID\", \"VITAL_SIGN_TYPE\", \"VITAL_SIGN_TAKEN_TIME\"]\n",
    "    )\n",
    "\n",
    "cedars_vitals[\"VITAL_SIGN_VALUE\"] = cedars_vitals[\"VITAL_SIGN_VALUE\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BMI\n",
    "\n",
    "bmi_df = pd.DataFrame({column: {} for column in cedars_vitals.columns})\n",
    "\n",
    "# for a patient, if they had weight and height emasured on same day, calculate BMI\n",
    "weights = cedars_vitals.loc[cedars_vitals['VITAL_SIGN_TYPE'] == 'Weight'].copy()\n",
    "heights = cedars_vitals.loc[cedars_vitals['VITAL_SIGN_TYPE'] == 'Height'].copy()\n",
    "\n",
    "weights[\"VITAL_SIGN_TAKEN_TIME\"] = pd.to_datetime(weights[\"VITAL_SIGN_TAKEN_TIME\"])\n",
    "heights[\"VITAL_SIGN_TAKEN_TIME\"] = pd.to_datetime(heights[\"VITAL_SIGN_TAKEN_TIME\"])\n",
    "\n",
    "for patient in heights['IP_PATIENT_ID'].unique():\n",
    "    patient_heights = heights[heights['IP_PATIENT_ID'] == patient].copy()\n",
    "    patient_weights = weights[weights['IP_PATIENT_ID'] == patient].copy()\n",
    "\n",
    "    for j, weight in patient_weights.iterrows():\n",
    "        patient_heights['TIME_DIFF'] = patient_heights['VITAL_SIGN_TAKEN_TIME'] - weight['VITAL_SIGN_TAKEN_TIME']\n",
    "\n",
    "        selected_height = patient_heights[patient_heights['TIME_DIFF'] == patient_heights['TIME_DIFF'].min()]\n",
    "        bmi = 703/16*weight['VITAL_SIGN_VALUE']/selected_height['VITAL_SIGN_VALUE']**2\n",
    "\n",
    "        new = { 'IP_PATIENT_ID': weight['IP_PATIENT_ID'],\n",
    "               'INPATIENT_DATA_ID':weight['INPATIENT_DATA_ID'],\n",
    "                    'VITAL_SIGN_TAKEN_TIME': weight['VITAL_SIGN_TAKEN_TIME'],\n",
    "                    'VITAL_SIGN_TYPE': 'BMI',\n",
    "                    'VITAL_SIGN_VALUE' : bmi\n",
    "        }\n",
    "        new = pd.DataFrame(new)\n",
    "\n",
    "        bmi_df = pd.concat([bmi_df, new])\n",
    "\n",
    "# bmi = weight.merge(height, on=['IP_PATIENT_ID', 'VITAL_SIGN_TAKEN_TIME'], how='inner')\n",
    "# bmi['BMI'] = 703/16*bmi['VITAL_SIGN_VALUE_x']/bmi['VITAL_SIGN_VALUE_y']**2\n",
    "# bmi = bmi.drop(['INPATIENT_DATA_ID_y','VITAL_SIGN_TYPE_y','VITAL_SIGN_TYPE_x','VITAL_SIGN_VALUE_y','VITAL_SIGN_VALUE_x'], axis=1)\n",
    "# bmi = bmi.rename(columns={'INPATIENT_DATA_ID_x':'INPATIENT_DATA_ID' ,'VITAL_SIGN_TAKEN_TIME': 'VITAL_SIGN_TAKEN_TIME', 'BMI': 'VITAL_SIGN_VALUE'})\n",
    "# bmi['VITAL_SIGN_TYPE'] = 'BMI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Merge weight and height only for measurements that occured for the same patients at the same time\n",
    "# Since weight and height are located the same column (VITAL_SIGN_VALUE), we must remember weight as VITAL_SIGN_VALUE_x and height as VITAL_SIGN_VALUE_y\n",
    "bmi = weight.merge(\n",
    "    height, on=[\"IP_PATIENT_ID\", \"VITAL_SIGN_TAKEN_TIME\"], how=\"inner\"\n",
    ")\n",
    "\n",
    "# Calculate BMI as 703*weight_in_lb/height_in_inch^2\n",
    "bmi[\"BMI\"] = 703 / 16 * bmi[\"VITAL_SIGN_VALUE_x\"] / bmi[\"VITAL_SIGN_VALUE_y\"] ** 2\n",
    "\n",
    "# Drop unecessary columns due to the merge. Can probably be done in a cleaner way\n",
    "bmi = bmi.drop(\n",
    "    [\n",
    "        \"INPATIENT_DATA_ID_y\",\n",
    "        \"VITAL_SIGN_TYPE_y\",\n",
    "        \"VITAL_SIGN_TYPE_x\",\n",
    "        \"VITAL_SIGN_VALUE_y\",\n",
    "        \"VITAL_SIGN_VALUE_x\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Rename to align with the original dataframe\n",
    "bmi = bmi.rename(\n",
    "    columns={\"INPATIENT_DATA_ID_x\": \"INPATIENT_DATA_ID\", \"BMI\": \"VITAL_SIGN_VALUE\"}\n",
    ")\n",
    "\n",
    "# Fill out a the VITAL_SIGN_TYPE column with 'BMI' since all values are BMI\n",
    "bmi[\"VITAL_SIGN_TYPE\"] = \"BMI\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_vitals = pd.concat([cedars_vitals, bmi_df])\n",
    "assess_value_alignment(cedars_vitals, ucla_vitals, cols=['VITAL_SIGN_TYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_vitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "\n",
    "number = []\n",
    "frequency = []\n",
    "ks = []\n",
    "\n",
    "for i, patient in enumerate(sorted(cedars_vitals['IP_PATIENT_ID'].unique())):\n",
    "    \n",
    "    patient_subset = cedars_vitals[cedars_vitals['IP_PATIENT_ID']==patient]\n",
    "    number.append(len(patient_subset))\n",
    "    times = sorted(pd.DatetimeIndex(patient_subset['VITAL_SIGN_TAKEN_TIME']))\n",
    "    times = sorted(list(set(times)))\n",
    "    \n",
    "    if len(times) < 2:\n",
    "        frequency.append(0)\n",
    "        ks.append(0)\n",
    "        continue\n",
    "    \n",
    "    total_distribution = []\n",
    "    for x in times:\n",
    "        total_distribution.append((x-times[0]).total_seconds()/3600)\n",
    "\n",
    "    differences = []\n",
    "    for x, y in zip(times[0::], times[1::]):\n",
    "        differences.append((y-x).total_seconds()/(3600))\n",
    "\n",
    "    frequency.append(np.mean(differences))\n",
    "    ks.append(stats.kurtosis(total_distribution))\n",
    "    \n",
    "    if i < 5:\n",
    "        print(differences)\n",
    "        print(total_distribution)\n",
    "        print(ks[-1])\n",
    "        all_freq = np.array(total_distribution)[:,None]\n",
    "        X_plot = np.linspace(-5, 20, 1000)[:, np.newaxis]\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=1.0).fit(all_freq)\n",
    "        log_dens = kde.score_samples(X_plot)\n",
    "        plt.plot(\n",
    "            X_plot[:, 0],\n",
    "            np.exp(log_dens)\n",
    "        )\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.DataFrame({'IP_PATIENT_ID': sorted(cedars_vitals['IP_PATIENT_ID'].unique()),\n",
    "                     'VISITS': number,\n",
    "                     'FREQ': frequency,\n",
    "                     'KS':ks,\n",
    "                     })\n",
    "\n",
    "display(freq['VISITS'].describe())\n",
    "display(freq['FREQ'].describe())\n",
    "display(freq['KS'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "\n",
    "number = []\n",
    "frequency = []\n",
    "ks = []\n",
    "\n",
    "for i, patient in enumerate(sorted(ucla_vitals['IP_PATIENT_ID'].unique())):\n",
    "    \n",
    "    patient_subset = ucla_vitals[ucla_vitals['IP_PATIENT_ID']==patient]\n",
    "    number.append(len(patient_subset))\n",
    "    times = sorted(pd.DatetimeIndex(patient_subset['VITAL_SIGN_TAKEN_TIME']))\n",
    "    times = sorted(list(set(times)))\n",
    "    \n",
    "    if len(times) < 2:\n",
    "        frequency.append(0)\n",
    "        ks.append(0)\n",
    "        continue\n",
    "    \n",
    "    total_distribution = []\n",
    "    for x in times:\n",
    "        total_distribution.append((x-times[0]).total_seconds()/3600)\n",
    "\n",
    "    differences = []\n",
    "    for x, y in zip(times[0::], times[1::]):\n",
    "        differences.append((y-x).total_seconds()/(3600))\n",
    "\n",
    "    frequency.append(np.mean(differences))\n",
    "    ks.append(stats.kurtosis(total_distribution))\n",
    "    \n",
    "    if i < 5:\n",
    "        print(differences)\n",
    "        print(total_distribution)\n",
    "        print(ks[-1])\n",
    "        all_freq = np.array(total_distribution)[:,None]\n",
    "        X_plot = np.linspace(-5, 20, 1000)[:, np.newaxis]\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=1.0).fit(all_freq)\n",
    "        log_dens = kde.score_samples(X_plot)\n",
    "        plt.plot(\n",
    "            X_plot[:, 0],\n",
    "            np.exp(log_dens)\n",
    "        )\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.DataFrame({'IP_PATIENT_ID': sorted(ucla_vitals['IP_PATIENT_ID'].unique()),\n",
    "                     'VISITS': number,\n",
    "                     'FREQ': frequency,\n",
    "                     'KS':ks,\n",
    "                     })\n",
    "\n",
    "display(freq['VISITS'].describe())\n",
    "display(freq['FREQ'].describe())\n",
    "display(freq['KS'].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "* UCLA has an explicit measure for BMI. Cedars does not have this, but it can be calculated from height and weight. The rule is that when there are measurements for height and weight at the EXACT same time, calculate the BMI. I found that we should be strict about time, because there are cases where weight can be measured multiple times a day with changes\n",
    "* UCLA is more balanced between measures\n",
    "* Cedars has fewer tempearture, weight, height, and thus BMI measures\n",
    "* Of the 482 height measures, 424 have corresponding weight measures at the same time\n",
    "\n",
    "\n",
    "*  whenever we have height, we can characterize BMI\n",
    "* check average frequency of patient/ fixed frequency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,15])\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "pie_plot(cedars_vitals['VITAL_SIGN_TYPE'].value_counts().reindex(ucla_vitals['VITAL_SIGN_TYPE'].value_counts().index), title=\"Cedars\")\n",
    "print(cedars_vitals['VITAL_SIGN_TYPE'].value_counts(normalize=True))\n",
    "plt.subplot(1,2,2)\n",
    "pie_plot(ucla_vitals['VITAL_SIGN_TYPE'].value_counts(), title=\"UCLA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cedars_vitals))\n",
    "print(len(cedars_vitals['IP_PATIENT_ID'].unique()))\n",
    "print(len(ucla_vitals))\n",
    "print(len(ucla_vitals['IP_PATIENT_ID'].unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_proc = data.load.read_files_and_combine(['Procedures.txt'], args.cedars_crrt_data_dir)\n",
    "ucla_proc = data.load.read_files_and_combine(['Procedures.txt'], args.ucla_crrt_data_dir)\n",
    "proc_mapping = pd.read_excel(os.path.join(args.cedars_crrt_data_dir, 'karumanchi_00001867_proc_code_bridge_table.xlsx'))\n",
    "\n",
    "# Cedars alignment. Assume contact date is diagnosis date since that's the date we have\n",
    "cedars_proc = cedars_proc.rename(\n",
    "        {\n",
    "        # Control\n",
    "        \"PROCEDURE_CODE\": \"PROC_CODE\", \n",
    "        \"PROCEDURE_DATE\": \"PROC_DATE\",\n",
    "\n",
    "        # Cedars \n",
    "        \"PROC_START_TIME\": \"PROC_DATE\"}, axis=1\n",
    "    )\n",
    "\n",
    "cedars_proc['PROC_CODE'] = cedars_proc['PROC_CODE'].astype(str) # Already CPT: {99195, 93925, 92950}\n",
    "ucla_proc['PROC_CODE'] = ucla_proc['PROC_CODE'].astype(str)\n",
    "proc_mapping['PROC_CODE'] = proc_mapping['PROC_CODE'].astype(str)\n",
    "proc_mapping['OT_PROC_CODE'] = proc_mapping['OT_PROC_CODE'].astype(str)\n",
    "\n",
    "assess_value_alignment(cedars_proc, ucla_proc, cols=['PROC_CODE'], len_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Mapping\n",
    "* Naive mapping just uses PROC_CODE as the key and OT_PROC_CODE as the value\n",
    "* Note that OT_CODE_TYPE can have different values: CPT, NaN, CUSTOM, HCPCS, ASA, ADA, HIPPS\n",
    "* We naively consider everything a CPT code if it is not CUSTOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(proc_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt_mapping = proc_mapping\n",
    "unique_codes = set(cedars_proc['PROC_CODE'].unique()).difference(set(cpt_mapping['PROC_CODE'].unique()))\n",
    "intersection = set(cedars_proc['PROC_CODE'].unique()).intersection(set(cpt_mapping['PROC_CODE'].unique()))\n",
    "\n",
    "print('Number of Mappings: ', len(proc_mapping))\n",
    "print('Number of Unique Cedars Codes that exist in mapping file', len(intersection))\n",
    "print('Number of Unique Cedars Codes that do not exist in mapping file: ', len(unique_codes))\n",
    "\n",
    "cpt_mapping = proc_mapping[~proc_mapping['OT_CODE_TYPE'].isin(['CUSTOM'])]\n",
    "unique_codes = set(cedars_proc['PROC_CODE'].unique()).difference(set(cpt_mapping['PROC_CODE'].unique()))\n",
    "intersection = set(cedars_proc['PROC_CODE'].unique()).intersection(set(cpt_mapping['PROC_CODE'].unique()))\n",
    "cpt_mapping = cpt_mapping[cpt_mapping['PROC_CODE'].isin(cedars_proc['PROC_CODE'].unique())]\n",
    "\n",
    "print('Number of Unique Cedars Codes with direct CPT Mapping: ', len(intersection))\n",
    "print('Number of Unique Cedars Codes without direct CPT Mapping: ', len(unique_codes))\n",
    "\n",
    "mapping_dict_naive = dict(\n",
    "    zip(cpt_mapping['PROC_CODE'], cpt_mapping['OT_PROC_CODE'])\n",
    ")\n",
    "\n",
    "cedars_proc[\"PROC_CODE\"] = cedars_proc[\"PROC_CODE\"].replace(\n",
    "    mapping_dict_naive\n",
    "    )\n",
    "\n",
    "assess_value_alignment(cedars_proc, ucla_proc, cols=['PROC_CODE'], len_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes \n",
    "* We see that there are still some custom cedars codes with no mapping to CPT codes\n",
    "* Also, it seems that (see below), there are cases when a Cedars code maps to both a Cedars code AND a CPT code. \n",
    "* The bad case below shows a case when there is no CPT code, but there are some cases where there are CPT codes but maybe they are also 'CUSTOM' type, and the mapping fails because the first mapping automatically obtained by pandas is the wrong code\n",
    "* To fix this below, we individually find ALL the mappings for each Cedars procedure code, and then check the OT_PROC_CODE if it is a CPT code (regardless of OT_CODE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(proc_mapping[proc_mapping['PROC_CODE'] == 'IMG117'])# GOOD example\n",
    "display(proc_mapping[proc_mapping['PROC_CODE'] == 'IMG1534']) # BAD example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non Naive\n",
    "* We individually find ALL the mappings for each Cedars procedure code, and then check the OT_PROC_CODE if it is a CPT code (regardless of OT_CODE_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hcuppy.cpt import CPT\n",
    "from pandas import DataFrame\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def trace_code_connections(starting_code: str, mapping_df: DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find all the different codes that a starting code is related to, as documented in the mapping_df\n",
    "    Each PROC_CODE is associated with an OT_PROC_CODE, but mapping is not one-to-one nor one-way\n",
    "    Perform a depth first search to find all codes that a given code is 'connected' to\n",
    "    \"\"\"\n",
    "\n",
    "    cpt = CPT()\n",
    "\n",
    "    found_code = \"na\"\n",
    "\n",
    "    visited_codes = set()\n",
    "    codes_to_visit = {starting_code}\n",
    "\n",
    "    while len(codes_to_visit) > 0:\n",
    "\n",
    "        current_code = codes_to_visit.pop()\n",
    "\n",
    "        if current_code not in visited_codes:\n",
    "\n",
    "            cpt_code = cpt.get_cpt_section(current_code)\n",
    "\n",
    "            # If there is one cpt code, store it\n",
    "            if cpt_code[\"sect\"] != \"na\":\n",
    "                found_code = current_code\n",
    "                break\n",
    "\n",
    "            # get related codes from OT_PROC_CODE when the current code is PROC_CODE\n",
    "            # AND get related codes from PROC_CODE when the current code is OT_PROC_CODE\n",
    "            neighbours_forwards = mapping_df[mapping_df[\"PROC_CODE\"] == current_code][\n",
    "                \"OT_PROC_CODE\"\n",
    "            ].to_list()\n",
    "            neighbours_backwards = mapping_df[\n",
    "                mapping_df[\"OT_PROC_CODE\"] == current_code\n",
    "            ][\"PROC_CODE\"].to_list()\n",
    "\n",
    "            neighbours_forwards = [x for x in neighbours_forwards if str(x) != \"nan\"]\n",
    "            neighbours_backwards = [x for x in neighbours_forwards if str(x) != \"nan\"]\n",
    "            additions = set(neighbours_backwards).union(set(neighbours_forwards))\n",
    "\n",
    "            codes_to_visit = set(codes_to_visit).union(additions)\n",
    "\n",
    "            visited_codes.add(current_code)\n",
    "\n",
    "    return found_code\n",
    "\n",
    "original_codes = []\n",
    "cpt_codes = []\n",
    "\n",
    "for i, code in enumerate(cedars_proc['PROC_CODE'].unique()):\n",
    "\n",
    "    # Trace all codes\n",
    "    found_code = trace_code_connections(code, proc_mapping)\n",
    "    \n",
    "    original_codes.append(code)\n",
    "    cpt_codes.append(found_code)\n",
    "\n",
    "mapping_dict = dict(\n",
    "    zip(original_codes, cpt_codes)\n",
    ")\n",
    "\n",
    "cedars_proc[\"PROC_CODE\"] = cedars_proc[\"PROC_CODE\"].replace(\n",
    "    mapping_dict\n",
    "    )\n",
    "\n",
    "assess_value_alignment(cedars_proc, ucla_proc, cols=['PROC_CODE'], len_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* The number of unique Cedars codes drops, showing many more now have mappings to CPT codes.\n",
    "* But the number of intersecting codes does not increase. Which means that many of the Cedars codes map to the same CPT codes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hcuppy.cpt import CPT\n",
    "from pickle import load\n",
    "from os.path import isfile\n",
    "from pandas import DataFrame\n",
    "\n",
    "def map_proc_code_to_cpt(\n",
    "    static_df: DataFrame,\n",
    "    raw_data_dir: str,\n",
    "    proc_mapping_file: str = \"Procedures_Code_Mapping.pkl\",\n",
    ") -> DataFrame:\n",
    "\n",
    "    # Should only do for Cedars\n",
    "    if not isfile(join(raw_data_dir, proc_mapping_file)):\n",
    "        return static_df\n",
    "\n",
    "    with open(join(raw_data_dir, proc_mapping_file), \"rb\") as f:\n",
    "        loaded_dict = load(f)\n",
    "\n",
    "    static_df[\"PROC_CODE\"] = static_df[\"PROC_CODE\"].astype(str)\n",
    "\n",
    "    static_df[\"PROC_CODE\"] = static_df[\"PROC_CODE\"].replace(loaded_dict)\n",
    "\n",
    "    return static_df\n",
    "\n",
    "cedars_proc = map_proc_code_to_cpt(cedars_proc, args.cedars_crrt_data_dir)\n",
    "\n",
    "cpt = CPT()\n",
    "\n",
    "cedars_proc_cpt = data.longitudinal_utils.hcuppy_map_code(\n",
    "        cedars_proc,\n",
    "        code_col=\"PROC_CODE\",\n",
    "        exploded_cols=[\"CPT_SECTION\", \"SECTION_DESCRIPTION\"],\n",
    "        hcuppy_converter_function=cpt.get_cpt_section,\n",
    "    )\n",
    "\n",
    "ucla_proc_cpt = data.longitudinal_utils.hcuppy_map_code(\n",
    "        ucla_proc,\n",
    "        code_col=\"PROC_CODE\",\n",
    "        exploded_cols=[\"CPT_SECTION\", \"SECTION_DESCRIPTION\"],\n",
    "        hcuppy_converter_function=cpt.get_cpt_section,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_proc_code = set(cedars_proc['PROC_CODE'])\n",
    "ucla_proc_code = set(ucla_proc['PROC_CODE'])\n",
    "\n",
    "only_cedars = cedars_proc_code.difference(ucla_proc_code)\n",
    "only_ucla = ucla_proc_code.difference(cedars_proc_code)\n",
    "both_diag = cedars_proc_code.intersection(ucla_proc_code)\n",
    "\n",
    "print('Rows in Cedars Diagnosis File: ', len(cedars_proc))\n",
    "print('Rows in Cedars Diagnosis File with codes unique from UCLA: ', \n",
    "      len(cedars_proc[cedars_proc['PROC_CODE'].isin(only_cedars)]), '|',\n",
    "      len(cedars_proc[cedars_proc['PROC_CODE'].isin(only_cedars)])/len(cedars_proc))\n",
    "print('Rows in Cedars Diagnosis File with codes shared with UCLA: ', \n",
    "      len(cedars_proc[cedars_proc['PROC_CODE'].isin(both_diag)]),'|',\n",
    "      len(cedars_proc[cedars_proc['PROC_CODE'].isin(both_diag)])/len(cedars_proc))\n",
    "\n",
    "print('Rows in UCLA Diagnosis File: ', len(ucla_proc))\n",
    "print('Rows in UCLA Diagnosis File with codes unique from Cedars: ', \n",
    "      len(ucla_proc[ucla_proc['PROC_CODE'].isin(only_ucla)]),'|',\n",
    "      len(ucla_proc[ucla_proc['PROC_CODE'].isin(only_ucla)])/len(ucla_proc))\n",
    "print('Rows in UCLA Diagnosis File with codes shared with Cedars: ', \n",
    "      len(ucla_proc[ucla_proc['PROC_CODE'].isin(both_diag)]),'|',\n",
    "      len(ucla_proc[ucla_proc['PROC_CODE'].isin(both_diag)])/len(ucla_proc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Cedars Rows: \", len(cedars_proc_cpt))\n",
    "print(\"Number of Cedars Unique CPT: \", len(cedars_proc_cpt[\"CPT_SECTION\"].unique()))\n",
    "print(cedars_proc_cpt[\"CPT_SECTION\"].value_counts())\n",
    "\n",
    "print(\"Number of UCLA Rows: \", len(ucla_proc_cpt))\n",
    "print(\"Number of UCLA Unique CPT: \",len(ucla_proc_cpt[\"CPT_SECTION\"].unique()))\n",
    "print(ucla_proc_cpt[\"CPT_SECTION\"].value_counts())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* Of the ~1 million rows, about 700 thousand are mapped to actual cpt codes\n",
    "* The variety of CPT codes from cedars is much lower"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_meds = data.load.read_files_and_combine(['Medications.txt'], args.cedars_crrt_data_dir)\n",
    "ucla_meds = data.load.read_files_and_combine(['Medications.txt'], args.ucla_crrt_data_dir)\n",
    "\n",
    "cedars_meds = cedars_meds.dropna(subset=[\"PHARM_SUBCLASS\"])\n",
    "ucla_meds = ucla_meds.dropna(subset=[\"PHARM_SUBCLASS\"])\n",
    "\n",
    "for column in  ['PHARM_SUBCLASS']:\n",
    "    ucla_meds[column] = ucla_meds[column].str.upper()\n",
    "for column in  ['PHARM_SUBCLASS']:\n",
    "    cedars_meds[column] = cedars_meds[column].str.upper()\n",
    "\n",
    "cedars_meds = cedars_meds.rename({\n",
    "                          \"ORDERING_DATE\": \"ORDER_DATE\",\n",
    "                          \"NAME\": 'MEDICATION_NAME'}, axis=1)\n",
    "\n",
    "ucla_meds = ucla_meds.rename({\"MEDISPAN_CLASS_NAME\": \"PHARM_CLASS\",\n",
    "                          \"ORDERING_DATE\": \"ORDER_DATE\"}, axis=1)\n",
    "\n",
    "ucla_meds = ucla_meds[~ucla_meds['PHARM_SUBCLASS'].str.contains('EACH')] #277\n",
    "ucla_meds = ucla_meds[~ucla_meds['PHARM_SUBCLASS'].str.isnumeric()] # 251\n",
    "\n",
    "assess_value_alignment(cedars_meds, ucla_meds, cols=['PHARM_SUBCLASS'], len_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = (set(cedars_meds['PHARM_SUBCLASS'].unique())).difference(set(ucla_meds['PHARM_SUBCLASS'].unique()))\n",
    "right = set(ucla_meds['PHARM_SUBCLASS'].unique())\n",
    "distsort = fuzzy_pairwise_diff(left, right, mode='sort')\n",
    "distpart = fuzzy_pairwise_diff(left, right, mode='partial')\n",
    "dist = fuzzy_pairwise_diff(left, right)\n",
    "print(distsort)\n",
    "print(distpart)\n",
    "print(dist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* 'sorted' distance looks for strings that have the same words but shifted. If these are very similar, then we consider them the same\n",
    "* 'partial' distance looks for strings that are subsets of another. These need review\n",
    "* Then we consider singular character differences. These also need review, but can be the same (e.g., singular vs plural) BUT also very different (e.g., Indirect vs Direct)\n",
    "\n",
    "* What I do below is that I look at the three different string matchings, and if similarity is over 0.9, i consider it a good mapping. I save the output, and then manually inspect for required changes. For the medications without mapping, I manually looked for mappings by considering both the top choices from the similarity metrics above (maybe they just barely were below the threshold) or I manually looked through the UCLA medications\n",
    "\n",
    "* From the 245 unique cedars medications, about 100 are automatically mapped, about 100 needed manual mapping, and about 40 do not have mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "import pickle \n",
    "\n",
    "def create_mapping_dict(left, right):\n",
    "\n",
    "    exceptions = {\n",
    "        \"ANTIDEPRESSANT - SEROTONIN-2 ANTAGONIST-REUPTAKE INHIBITORS (SARIS)\",\n",
    "        \"ANTIDEPRESSANT-NOREPINEPHRINE AND DOPAMINE REUPTAKE INHIBITORS (NDRIS)\",\n",
    "        \"BLOOD CELL AND PLATELET DISORDER TX-SPLEEN TYROSINE KINASE INHIBITORS\",\n",
    "        \"HYPERURICEMIA THERAPY - XANTHINE OXIDASE INHIBITORS\",\n",
    "        \"NARCOLEPSY THERAPY AGENTS - NON-SYMPATHOMIMETIC\",\n",
    "        \"URINARY RETENTION THERAPY - PARASYMPATHOMIMETIC AGENTS\",\n",
    "        \"CALCIMIMETIC, PARATHYROID CALCIUM RECEPTOR SENSITIVITY ENHANCER\",\n",
    "    }\n",
    "\n",
    "    manual_mapping = {\n",
    "        \"CALCIUM CHANNEL BLOCKERS - DIHYDROPYRIDINES - CEREBROVASCULAR SPECIFIC\": \"CALCIUM CHANNEL BLOCKERS\",\n",
    "        \"CALCIUM CHANNEL BLOCKERS - BENZOTHIAZEPINES\": \"CALCIUM CHANNEL BLOCKERS\",\n",
    "        \"BETA BLOCKERS NON-CARDIAC SELECTIVE\": \"BETA BLOCKERS NON-SELECTIVE\",\n",
    "        \"ASTHMA THERAPY - LEUKOTRIENE RECEPTOR ANTAGONISTS\": \"LEUKOTRIENE MODULATORS\",\n",
    "        \"CMV ANTIVIRAL AGENT - INORGANIC PYROPHOSPHATE ANALOGS\": \"CMV AGENTS\",\n",
    "        \"OVERACTIVE BLADDER AGENTS - BETA -3 ADRENERGIC RECEPTOR AGONIST\": \"URINARY ANTISPASMODICS - BETA-3 ADRENERGIC AGONISTS 61\",\n",
    "        \"ANTIPARKINSON - DOPAMINERG-PERIPHERAL DOPA-DECARBOXYLASE INHIBIT COMB\": \"ANTIPARKINSON DOPAMINERGICS\",\n",
    "        \"CONTRAST MEDIA - BARIUM\": \"RADIOGRAPHIC CONTRAST MEDIA\",\n",
    "        \"ANTIPSYCHOTIC - ATYPICAL DOPAMINE-SEROTONIN ANTAG- BENZISOTHIAZOLONES\": \"BENZODIAZEPINE ANTAGONISTS\",\n",
    "        \"THROMBOLYTIC - TISSUE PLASMINOGEN ACTIVATORS\": \"THROMBOLYTIC ENZYMES\",\n",
    "        \"ANTICONVULSANT - BARBITURATES AND DERIVATIVES\": \"ANTICONVULSANTS - MISC.\",\n",
    "        \"ANTIHISTAMINES - 2ND GENERATION\": \"ANTIHISTAMINES - NON-SEDATING\",\n",
    "        \"CONTRAST MEDIA - MAGNETIC RESONANCE GADOLINIUM COMPLEXES\": \"RADIOGRAPHIC CONTRAST MEDIA\",\n",
    "        \"ANTIDEPRESSANT-TRICYCLICS AND RELATED (NON-SELECT REUPTAKE INHIBITORS)\": \"TRICYCLIC AGENTS\",\n",
    "        \"ANTIARRHYTHMIC - CLASS IV\": \"ANTIARRHYTHMICS - MISC.\",\n",
    "        \"AGENTS FOR OPIOID WITHDRAWAL, OPIOID-TYPE\": \"OPIOID PARTIAL AGONISTS\",\n",
    "        \"ANTIANXIETY AGENT - ANTIHISTAMINE TYPE\": \"ANTIANXIETY AGENTS - MISC.\",\n",
    "        \"ANTISEPTIC - OXIDIZING AGENTS\": \"ANTISEPTICS & DISINFECTANTS\",\n",
    "        \"VACCINE VIRAL - INFLUENZA A AND B\": \"VIRAL VACCINES\",\n",
    "        \"CONTRAST MEDIA - IODINATED NONIONIC\": \"RADIOGRAPHIC CONTRAST MEDIA\",\n",
    "        \"DERMATOLOGICAL - PROTECTANTS\": \"MISC. DERMATOLOGICAL PRODUCTS\",\n",
    "        \"DERMATOLOGICAL - ANTIBACTERIAL MIXTURES\": \"MISC. DERMATOLOGICAL PRODUCTS\",\n",
    "        \"ANTIDOTE - CYANIDE POISONING\": \"ANTIDOTES - CHELATING AGENTS\",\n",
    "        \"PENICILLIN ANTIBIOTIC - NATURAL\": \"NATURAL PENICILLINS\",\n",
    "        \"ANTISEPTIC - CHLORINE RELEASING\": \"CHLORINE ANTISEPTICS\",\n",
    "        \"DEXTROSE SOLUTIONS\": \"PERITONEAL DIALYSIS SOLUTIONS\",\n",
    "        \"DERMATOLOGICAL - ENZYME COMBINATIONS OTHER\": \"ENZYMES\",\n",
    "        \"ANTIANXIETY AGENT - NON-BENZODIAZEPINE\": \"ANTIANXIETY AGENTS - MISC.\",\n",
    "        \"ANTICONVULSANT - CARBOXYLIC ACID DERIVATIVES\": \"ANTICONVULSANTS - MISC.\",\n",
    "        \"ANTIARRHYTHMIC - CLASS IB\": \"ANTIARRHYTHMICS - MISC.\",\n",
    "        \"CEPHALOSPORIN ANTIBIOTICS - 1ST GENERATION\": \"CEPHALOSPORINS - 1ST GENERATION\",\n",
    "        \"LAXATIVE - BULK FORMING\": \"BULK LAXATIVES\",\n",
    "        \"ASTHMA THERAPY - INHALED CORTICOSTEROIDS (GLUCOCORTICOIDS)\": \"GLUCOCORTICOSTEROIDS\",\n",
    "        \"ASTHMA/COPD THERAPY - BETA ADRENERGIC-GLUCOCORTICOID COMBINATIONS\": \"GLUCOCORTICOSTEROIDS\",\n",
    "        \"DERMATOLOGICAL - GLUCOCORTICOID\": \"GLUCOCORTICOSTEROIDS\",\n",
    "        \"ANTINEOPLASTIC - CD20 SPECIFIC RECOMBINANT MONOCLONAL ANTIBODY AGENTS\": \"MONOCLONAL ANTIBODIES\",\n",
    "        \"ANTINEOPLASTIC - PHOTOSENSITIZERS\": \"ANTINEOPLASTICS MISC.\",\n",
    "        \"ACNE THERAPY TOPICAL - ANTI-INFECTIVE\": \"ANTI-INFECTIVE AGENTS - MISC.\",\n",
    "        \"ANALGESIC OPIOID HYDROMORPHONE COMBINATIONS\": \"ANALGESIC COMBINATIONS\",\n",
    "        \"CONTRAST MEDIA - IODINATED IONIC\": \"RADIOGRAPHIC CONTRAST MEDIA\",\n",
    "        \"ANTICONVULSANT - FUNCTIONALIZED AMINO ACID\": \"ANTICONVULSANTS - MISC.\",\n",
    "        \"ANTIARRHYTHMIC - CLASS II\": \"ANTIARRHYTHMICS - MISC.\",\n",
    "        \"MINERALS AND ELECTROLYTES - IODINE\": \"ELECTROLYTE MIXTURES\",\n",
    "        \"VITAMINS - B-1, THIAMINE AND DERIVATIVES\": \"B-COMPLEX VITAMINS\",\n",
    "        \"VITAMINS - B-6, PYRIDOXINE AND DERIVATIVES\": \"B-COMPLEX VITAMINS\",\n",
    "        \"VITAMINS - FOLIC ACID AND DERIVATIVES\": \"B-COMPLEX VITAMINS\",\n",
    "        \"ALTERNATIVE THERAPY - UNCLASSIFIED\": \"ALTERNATIVE MEDICINE - U\",\n",
    "        \"ANTIPSYCHOTIC - ATYPICAL DOPAMINE-SEROTONIN ANTAG-DIBENZODIAZEPINE DER\": \"ANTIPSYCHOTICS - MISC.\",\n",
    "        \"IMMUNOSUPPRESSIVE - CALCINEURIN INHIBITORS\": \"IMMUNOSUPPRESSIVE AGENTS\",\n",
    "        \"ANTIPARKINSON THERAPY - NON-ERGOT DOPAMINE AGONIST AGENTS\": \"ANTIPARKINSON DOPAMINERGICS\",\n",
    "        \"HEPATITIS C - NUCLEOSIDE ANALOGS\": \"HEPATITIS AGENTS\",\n",
    "        \"HEPATITIS B TREATMENT- NUCLEOSIDE ANALOGS (ANTIVIRAL)\": \"HEPATITIS AGENTS\",\n",
    "        \"ANALGESIC OPIOID CODEINE COMBINATIONS\": \"ANALGESIC COMBINATIONS\",\n",
    "        \"ANORECTAL - GLUCOCORTICOIDS\": \"GLUCOCORTICOSTEROIDS\",\n",
    "        \"IMMUNOSUPPRESSIVE - PURINE ANALOGS\": \"IMMUNOSUPPRESSIVE AGENTS\",\n",
    "        \"OPHTHALMIC - ANTI-INFLAMMATORY, NSAIDS\": \"NONSTEROIDAL ANTI-INFLAMMATORY AGENTS (NSAIDS)\",\n",
    "        \"RINGER'S AND LACTATED RINGER'S SOLUTIONS\": \"IRRIGATION SOLUTIONS\",\n",
    "        \"DEXTROSE AND LACTATED RINGER'S SOLUTIONS\": \"IRRIGATION SOLUTIONS\",\n",
    "        \"NSAID ANALGESIC, CYCLOOXYGENASE-2 (COX-2) SELECTIVE INHIBITORS\": \"NONSTEROIDAL ANTI-INFLAMMATORY AGENTS (NSAIDS)\",\n",
    "        \"CEPHALOSPORIN ANTIBIOTICS - 4TH GENERATION\": \"CEPHALOSPORINS - 4TH GENERATION\",\n",
    "        \"HEMOSTATIC TOPICAL AGENTS\": \"HEMOSTATICS - TOPICAL\",\n",
    "        \"OPHTHALMIC - ANTICHOLINERGICS\": \"OPHTHALMICS - MISC.\",\n",
    "        \"SEDATIVE-HYPNOTIC - GABA-RECEPTOR MODULATORS\": \"HORMONE RECEPTOR MODULATORS\",\n",
    "        \"MACROLIDE ANTIBIOTICS\": \"ANTIBIOTICS - TOPICAL\",\n",
    "        \"ANTIBACTERIAL FOLATE ANTAGONIST - OTHER COMBINATIONS\": \"ANTI-INFECTIVE MISC. - COMBINATIONS\",\n",
    "        \"CEPHALOSPORIN ANTIBIOTICS - 2ND GENERATION\": \"CEPHALOSPORINS - 2ND GENERATION\",\n",
    "        \"ANTIPSYCHOTIC -ATYPICAL DOPAMINE-SEROTONIN ANTAG-DIBENZOTHIAZEPINE DER\": \"ANTIPSYCHOTICS - MISC.\",\n",
    "        \"CEPHALOSPORIN ANTIBIOTICS - 3RD GENERATION\": \"CEPHALOSPORINS - 3RD GENERATION\",\n",
    "        \"GALLSTONE SOLUBILIZING (LITHOLYSIS) AGENTS\": \"GALLSTONE SOLUBILIZING AGENTS\",\n",
    "        \"AGENTS TO TREAT HYPOGLYCEMIA (HYPERGLYCEMICS)\": \"ANTIHYPERLIPIDEMICS - COMBINATIONS\",\n",
    "        \"DERMATOLOGICAL - ANTIBACTERIAL POVIDONE-IODINE PREPARATIONS\": \"MISC. DERMATOLOGICAL PRODUCTS\",\n",
    "        \"ANGIOTENSIN II RECEPTOR BLOCKERS (ARBS)\": \"ANGIOTENSIN II RECEPTOR ANTAGONISTS\",\n",
    "        \"ATTENTION DEFICIT-HYPERACTIVITY (ADHD) THERAPY, STIMULANT-TYPE\": \"ATTENTION-DEFICIT/HYPERACTIVITY DISORDER (ADHD) AGENTS\",\n",
    "        \"ANTICONVULSANT - MONOSACCHARIDE DERIVATIVES\": \"ANTICONVULSANTS - MISC.\",\n",
    "        \"ANTICONVULSANT - IMINOSTILBENE DERIVATIVES\": \"ANTICONVULSANTS - MISC.\",\n",
    "        \"HERPES ANTIVIRAL AGENT - PURINE ANALOGS\": \"HERPES AGENTS\",\n",
    "        \"ANTIHYPERGLYCEMIC, INCRETIN MIMETIC,GLP-1 RECEPTOR AGONIST ANALOG-TYPE\": \"INCRETIN MIMETIC AGENTS (GLP-1 RECEPTOR AGONISTS)\",\n",
    "        \"PULMONARY FIBROSIS TREATMENT AGENTS - ANTIFIBROTIC THERAPY\": \"PULMONARY FIBROSIS AGENTS\",\n",
    "        \"CMV ANTIVIRAL AGENT - NUCLEOSIDE ANALOGS\": \"CMV AGENTS\",\n",
    "        \"GLUCOCORTICOIDS\": \"GLUCOCORTICOSTEROIDS\",\n",
    "        \"ANTIARRHYTHMIC - CLASS III\": \"ANTIARRHYTHMICS TYPE III\",\n",
    "        \"MU-OPIOID RECEPTOR ANTAGONISTS, PERIPHERALLY-ACTING\": \"PERIPHERAL OPIOID RECEPTOR ANTAGONISTS\",\n",
    "        \"GENERAL ANESTHETIC - PARENTERAL, PHENOL DERIVATIVES\": \"ANESTHETICS - MISC.\",\n",
    "        \"MINERALS AND ELECTROLYTES - PARENTERAL ELECTROLYTE COMBINATIONS\": \"ELECTROLYTE MIXTURES\",\n",
    "        \"LOW MOLECULAR WEIGHT HEPARINS\": \"HEPARINS AND HEPARINOID-LIKE AGENTS\",\n",
    "        \"ANTIEMETIC - CANNABINOID TYPE\": \"ANTIEMETICS - MISCELLANEOUS\",\n",
    "        \"ANTIEMETIC - SELECTIVE SEROTONIN 5-HT3 ANTAGONISTS\": \"5-HT3 RECEPTOR ANTAGONISTS\",\n",
    "        \"MINERALS AND ELECTROLYTES - PARENTERAL ELECTROLYTE COMBINATIONS\": \"ELECTROLYTE MIXTURES\",\n",
    "        \"OXYTOCIC - OXYTOCIN AND ANALOGS\": \"OXYTOCICS\",\n",
    "        \"NSAID ANALGESICS (COX NON-SPECIFIC) - PROPIONIC ACID DERIVATIVES\": \"NONSTEROIDAL ANTI-INFLAMMATORY AGENTS (NSAIDS)\",\n",
    "        \"DERMATOLOGICAL - PROTECTANT COMBINATIONS\": \"MISC. DERMATOLOGICAL PRODUCTS\",\n",
    "        \"HEMOSTATIC SYSTEMIC - ANTIFIBRINOLYTIC AGENTS\": \"HEMOSTATICS - SYSTEMIC\",\n",
    "        \"PULMONARY ANTIHYPERTENSIVE AGENTS-SOLUBLE GUANYLATE CYCLASE STIMULATOR\": \"PULMONARY HYPERTENSION - SOL GUANYLATE CYCLASE STIMULATOR\",\n",
    "        \"BIPOLAR THERAPY AGENTS - ATYPICAL ANTIPSYCHOTICS\": \"ANTIPSYCHOTICS - MISC.\",\n",
    "        \"ANALGESIC OR ANTIPYRETIC NON-OPIOID\": \"ANALGESICS OTHER\",\n",
    "        \"ANTIHISTAMINE - 1ST GENERATION - PIPERIDINES\": \"ANTIHISTAMINES - PIPERIDINES\",\n",
    "        \"LAXATIVE - SALINE AND OSMOTIC\": \"SALINE LAXATIVES\",\n",
    "        \"ANTACID - SIMETHICONE COMBINATIONS\": \"ANTACID COMBINATIONS\",\n",
    "        \"ANALGESIC OPIOID OXYCODONE COMBINATIONS\": \"ANALGESIC COMBINATIONS\",\n",
    "        \"DIURETIC - THIAZIDES AND RELATED\": \"THIAZIDES AND THIAZIDE-LIKE DIURETICS\",\n",
    "        \"NASAL CORTICOSTEROIDS\": \"NASAL STEROIDS\",\n",
    "        \"DERMATOLOGICAL - CALCINEURIN INHIBITORS\": \"MISC. DERMATOLOGICAL PRODUCTS\",\n",
    "        \"HYPERPARATHYROID TREATMENT AGENTS - VITAMIN D ANALOG-TYPE\": \"ANTITHYROID AGENTS\",\n",
    "        \"OPHTHALMIC - DIAGNOSTIC AGENTS\": \"OPHTHALMICS - MISC.\",\n",
    "        \"PLATELET AGGREGATION INHIB-PDESTERASE AND ADENOSINE DEAMINASE INHIBITR\": \"PLATELET AGGREGATION INHIBITORS\",\n",
    "        \"ANTIHISTAMINE - 1ST GENERATION - ETHANOLAMINES\": \"ANTIHISTAMINES - ETHANOLAMINES\",\n",
    "        \"ANTIDEPRESSANT - ALPHA-2 RECEPTOR ANTAGONISTS (NASSA)\": \"ALPHA-2 RECEPTOR ANTAGONISTS (TETRACYCLICS)\",\n",
    "        \"PHARMACEUTICAL ADJUVANT - FLAVORING AGENTS\": \"PHARMACEUTICAL EXCIPIENTS\",\n",
    "        \"ANTICONVULSANT - PYRROLIDINE DERIVATIVES\": \"ANTICONVULSANTS - MISC.\",\n",
    "        \"ANTIPROTOZOAL-ANTIBACTERIAL 1ST GENERATION 2-METHYL-5-NITROIMIDAZOLE\": \"ANTIPROTOZOAL AGENTS\",\n",
    "        \"PULMONARY ARTERIAL HYPERTENSION - SELECTIVE CGMP-PDE5 INHIBITORS\": \"PULMONARY HYPERTENSION - PHOSPHODIESTERASE INHIBITORS\",\n",
    "        \"CMV ANTIVIRAL AGENT - TERMINASE COMPLEX INHIBITORS\": \"CMV AGENTS\",\n",
    "    }\n",
    "\n",
    "    # Got these from running the algorithm. Just saving here to save time for future reference\n",
    "    mapping = {\n",
    "        \"CALCIUM CHANNEL BLOCKERS - DIHYDROPYRIDINES\": \"CALCIUM CHANNEL BLOCKERS\",\n",
    "        \"BETA BLOCKERS CARDIAC SELECTIVE\": \"BETA BLOCKERS CARDIO-SELECTIVE\",\n",
    "        \"INFLAMMATORY BOWEL AGENT - AMINOSALICYLATES AND RELATED AGENTS\": \"SALICYLATES\",\n",
    "        \"DERMATOLOGICAL - TOPICAL LOCAL ANESTHETIC AMIDES\": \"LOCAL ANESTHETICS - AMIDES\",\n",
    "        \"DEXTROSE AND SODIUM CHLORIDE SOLUTIONS\": \"SODIUM\",\n",
    "        \"MINERALS AND ELECTROLYTES - POTASSIUM FOR INJECTION\": \"POTASSIUM\",\n",
    "        \"ANTIFUNGAL - FLUORINATED PYRIMIDINE-TYPE AGENTS\": \"ANTIFUNGALS\",\n",
    "        \"DERMATOLOGICAL - LOCAL ANESTHETIC COMBINATIONS\": \"LOCAL ANESTHETIC COMBINATIONS\",\n",
    "        \"ANTIFUNGAL - GLUCAN SYNTHESIS INHIBITOR, ECHINOCANDINS\": \"ANTIFUNGAL - GLUCAN SYNTHESIS INHIBITORS\",\n",
    "        \"ANALGESIC OPIOID AGONISTS\": \"OPIOID AGONISTS\",\n",
    "        \"PHOSPHATE BINDERS\": \"PHOSPHATE\",\n",
    "        \"ANTIANGINAL - CORONARY VASODILATORS (NITRATES)\": \"NITRATES\",\n",
    "        \"ARTIFICIAL TEARS AND LUBRICANT SINGLE AGENTS\": \"ARTIFICIAL TEARS AND LUBRICANTS\",\n",
    "        \"ANTIANXIETY AGENT - BENZODIAZEPINES\": \"BENZODIAZEPINES\",\n",
    "        \"DERMATOLOGICAL - BURN PRODUCTS ANTI-INFECTIVE\": \"BURN PRODUCTS\",\n",
    "        \"CARDIOVASCULAR SYMPATHOMIMETICS\": \"SYMPATHOMIMETICS\",\n",
    "        \"DIURETIC - LOOP\": \"LOOP DIURETICS\",\n",
    "        \"MINERALS AND ELECTROLYTES - POTASSIUM, ORAL\": \"POTASSIUM\",\n",
    "        \"ANTACID - CALCIUM\": \"CALCIUM\",\n",
    "        \"DIURETIC - CARBONIC ANHYDRASE INHIBITORS\": \"CARBONIC ANHYDRASE INHIBITORS\",\n",
    "        \"METABOLIC MODIFIER - CARNITINE REPLENISHER AGENTS\": \"METABOLIC MODIFIERS\",\n",
    "        \"DIURETIC - SELECTIVE ARGININE VASOPRESSIN V2 RECEPTOR ANTAGONISTS\": \"VASOPRESSIN RECEPTOR ANTAGONISTS\",\n",
    "        \"OPHTHALMIC-INTRAOCULAR PRESSURE REDUCING AGENTS, PROSTAGLANDIN ANALOGS\": \"PROSTAGLANDINS\",\n",
    "        \"OPHTHALMIC ANTIBIOTIC - FLUOROQUINOLONES\": \"FLUOROQUINOLONES\",\n",
    "        \"INSULIN ANALOGS - RAPID ACTING\": \"INSULIN\",\n",
    "        \"MOUTH AND THROAT - ANTISEPTICS\": \"ANTISEPTICS - MOUTH/THROAT\",\n",
    "        \"MINERALS AND ELECTROLYTES - CALCIUM REPLACEMENT\": \"CALCIUM\",\n",
    "        \"DERMATOLOGICAL - TOPICAL LOCAL ANESTHETIC ESTERS\": \"LOCAL ANESTHETICS - ESTERS\",\n",
    "        \"SMOKING DETERRENTS - NICOTINE-TYPE\": \"SMOKING DETERRENTS\",\n",
    "        \"HEPARINS\": \"HEPARINS AND HEPARINOID-LIKE AGENTS\",\n",
    "        \"ABORTIFACIENTS OR CERVICAL RIPENING AGENTS - PROSTAGLANDIN ANALOGS\": \"PROSTAGLANDINS\",\n",
    "        \"URINARY ANTISPASMODIC - SMOOTH MUSCLE RELAXANTS\": \"ANTISPASMODICS\",\n",
    "        \"ANTICOAGULANTS - COUMARIN\": \"COUMARIN ANTICOAGULANTS\",\n",
    "        \"INDIRECT FACTOR XA INHIBITORS\": \"DIRECT FACTOR XA INHIBITORS\",\n",
    "        \"ANTIHISTAMINE - 1ST GENERATION - PHENOTHIAZINES\": \"PHENOTHIAZINES\",\n",
    "        \"ANTIPSYCHOTIC - BUTYROPHENONE DERIVATIVES\": \"BUTYROPHENONES\",\n",
    "        \"INSULINS\": \"INSULIN\",\n",
    "        \"ANTITUBERCULAR - ISONICOTINIC ACID DERIVATIVES\": \"NICOTINIC ACID DERIVATIVES\",\n",
    "        \"ARTIFICIAL TEARS AND LUBRICANT COMBINATIONS\": \"ARTIFICIAL TEARS AND LUBRICANTS\",\n",
    "        \"SYSTEMIC SYMPATHOMIMETIC DECONGESTANTS\": \"SYMPATHOMIMETIC DECONGESTANTS\",\n",
    "        \"MINERALS AND ELECTROLYTES - IRON\": \"IRON\",\n",
    "        \"AMINOGLYCOSIDE ANTIBIOTIC\": \"AMINOGLYCOSIDES\",\n",
    "        \"DMARD - PYRIMIDINE SYNTHESIS INHIBITORS\": \"PYRIMIDINE SYNTHESIS INHIBITORS\",\n",
    "        \"PLATELET AGGREGATION INHIBITORS - PHOSPHODIESTERASE III INHIBITORS\": \"PLATELET AGGREGATION INHIBITORS\",\n",
    "        \"DIAGNOSTIC RADIOPHARMACEUTICALS - RENAL IMAGING\": \"DIAGNOSTIC RADIOPHARMACEUTICALS\",\n",
    "        \"DIAGNOSTIC DRUGS - IN VIVO OTHER\": \"DIAGNOSTIC DRUGS\",\n",
    "        \"ANTIHYPERGLYCEMIC - SULFONYLUREA DERIVATIVES\": \"SULFONYLUREAS\",\n",
    "        \"ANTINEOPLASTIC - ANTIMETABOLITE - FOLIC ACID ANALOGS\": \"ANTIMETABOLITES\",\n",
    "        \"AMINOPENICILLIN ANTIBIOTIC\": \"AMINOPENICILLINS\",\n",
    "        \"OPHTHALMIC - BETA BLOCKERS-CARBONIC ANHYDRASE INHIBITOR COMBINATIONS\": \"CARBONIC ANHYDRASE INHIBITORS\",\n",
    "        \"CARBAPENEM ANTIBIOTICS (THIENAMYCINS)\": \"CARBAPENEMS\",\n",
    "        \"ANTACID - BICARBONATE\": \"ANTACIDS - BICARBONATE\",\n",
    "        \"DERMATOLOGICAL - EMOLLIENT MIXTURES\": \"EMOLLIENTS\",\n",
    "        \"ANTINEOPLASTIC ANTIBIOTIC - ANTHRACYCLINES\": \"ANTINEOPLASTIC ANTIBIOTICS\",\n",
    "        \"PROSTATIC HYPERTROPHY AGENT - ALPHA-1-ADRENOCEPTOR ANTAGONISTS\": \"PROSTATIC HYPERTROPHY AGENTS\",\n",
    "        \"ANTIDIURETIC AND VASOPRESSOR HORMONES\": \"VASOPRESSORS\",\n",
    "        \"LAXATIVE - LUBRICANT\": \"LUBRICANT LAXATIVES\",\n",
    "        \"SODIUM CHLORIDE, PARENTERAL\": \"SODIUM\",\n",
    "        \"THYROID HORMONES - SYNTHETIC T4 (THYROXINE)\": \"THYROID HORMONES\",\n",
    "        \"THYROID HORMONES - SYNTHETIC T3 (TRIIODOTHYRONINE)\": \"THYROID HORMONES\",\n",
    "        \"ANTIPSYCHOTIC - PHENOTHIAZINES, PIPERAZINE\": \"PHENOTHIAZINES\",\n",
    "        \"MINERALS AND ELECTROLYTES - MAGNESIUM\": \"MAGNESIUM\",\n",
    "        \"MUSCULOSKELETAL THERAPY AGENT - VISCOSUPPLEMENTS\": \"VISCOSUPPLEMENTS\",\n",
    "        \"ANTINEOPLASTIC - ANTIMETABOLITE - UREA DERIVATIVES\": \"ANTIMETABOLITES\",\n",
    "        \"TETRACYCLINE ANTIBIOTICS\": \"TETRACYCLINES\",\n",
    "        \"GI ANTISPASMODIC - BELLADONNA ALKALOIDS\": \"ANTISPASMODICS\",\n",
    "        \"ANTIPSYCHOTIC - ATYPICAL DOPAMINE-SEROTONIN ANTAG- BENZISOXAZOLE DERIV\": \"BENZISOXAZOLES\",\n",
    "        \"LINCOSAMIDE ANTIBIOTICS\": \"LINCOSAMIDES\",\n",
    "        \"ASTHMA/COPD -  PHOSPHODIESTERASE-4 (PDE4) INHIBITORS\": \"PHOSPHODIESTERASE 4 (PDE4) INHIBITORS\",\n",
    "        \"GASTRIC ACID SECRETION REDUCING AGENTS - PROTON PUMP INHIBITORS (PPIS)\": \"PROTON PUMP INHIBITORS\",\n",
    "        \"ANTIDIARRHEAL - ANTIPERISTALTIC AGENTS\": \"ANTIPERISTALTIC AGENTS\",\n",
    "        \"B-COMPLEX VITAMIN COMBINATIONS\": \"B-COMPLEX VITAMINS\",\n",
    "        \"ANTIRETROVIRAL-NUCLEOSIDE ANALOGS AND INTEGRASE INHIBITOR COMBINATIONS\": \"ANTIRETROVIRALS\",\n",
    "        \"ANTIEMETIC - PHENOTHIAZINES\": \"PHENOTHIAZINES\",\n",
    "        \"OPIOID ANTITUSSIVE-EXPECTORANT COMBINATIONS\": \"ANTITUSSIVES\",\n",
    "        \"ANTIHYPERLIPIDEMIC - HMG COA REDUCTASE INHIBITORS (STATINS)\": \"HMG COA REDUCTASE INHIBITORS\",\n",
    "        \"ANTIDEPRESSANT - SELECTIVE SEROTONIN REUPTAKE INHIBITORS (SSRIS)\": \"SELECTIVE SEROTONIN REUPTAKE INHIBITORS (SSRIS)\",\n",
    "        \"ANTIHYPERLIPIDEMIC - BILE ACID SEQUESTRANTS\": \"BILE ACID SEQUESTRANTS\",\n",
    "        \"SKELETAL MUSCLE RELAXANT - CENTRAL MUSCLE RELAXANTS\": \"CENTRAL MUSCLE RELAXANTS\",\n",
    "        \"SEDATIVE-HYPNOTIC - BENZODIAZEPINES\": \"BENZODIAZEPINES\",\n",
    "        \"ANTIRETROVIRAL - NON-NUCLEOSIDE REVERSE TRANSCRIPTASE INHIB (NNRTI)\": \"ANTIRETROVIRALS\",\n",
    "        \"MULTIVITAMIN AND MINERAL COMBINATIONS\": \"MINERAL COMBINATIONS\",\n",
    "        \"GLYCOPEPTIDE ANTIBIOTICS\": \"GLYCOPEPTIDES\",\n",
    "        \"DERMATOLOGICAL - EMOLLIENTS\": \"EMOLLIENTS\",\n",
    "        \"MIGRAINE THERAPY - SELECTIVE SEROTONIN AGONISTS 5-HT(1)\": \"SEROTONIN AGONISTS\",\n",
    "        \"MINERALS AND ELECTROLYTES - BICARBONATE PRODUCING OR CONTAINING AGENTS\": \"BICARBONATES\",\n",
    "        \"LOCAL ANESTHETIC - AMIDES\": \"LOCAL ANESTHETICS - AMIDES\",\n",
    "        \"AMINOPENICILLIN ANTIBIOTIC - BETA-LACTAMASE INHIBITOR COMBINATIONS\": \"AMINOPENICILLINS\",\n",
    "        \"URINARY ANTISPASMODIC - ANTICHOL., M(3) MUSCARINIC SELECTIVE (BLADDER)\": \"ANTISPASMODICS\",\n",
    "        \"DERMATOLOGICAL - ANTIFUNGAL-GLUCOCORTICOID COMBINATIONS\": \"ANTIFUNGALS\",\n",
    "        \"PULMONARY ARTERIAL HYPERTENSION - ENDOTHELIN RECEPTOR ANTAGONISTS\": \"PULMONARY HYPERTENSION - ENDOTHELIN RECEPTOR ANTAGONISTS\",\n",
    "        \"MINERALS AND ELECTROLYTES - ZINC\": \"ZINC\",\n",
    "        \"INSULIN ANALOGS - LONG ACTING\": \"INSULIN\",\n",
    "        \"LOCAL ANESTHETIC - SYMPATHOMIMETIC COMBINATIONS\": \"SYMPATHOMIMETICS\",\n",
    "        \"ANTIRETROVIRAL-NUCLEOSIDE, NUCLEOTIDE ANALOGS AND NON-NUCLEOSIDE RTI\": \"ANTIRETROVIRALS\",\n",
    "        \"OPHTHALMIC - CARBONIC ANHYDRASE INHIBITORS\": \"CARBONIC ANHYDRASE INHIBITORS\",\n",
    "        \"ANTIFUNGAL - AMPHOTERIC POLYENE MACROLIDES\": \"ANTIFUNGALS\",\n",
    "        \"OPHTHALMIC - LOCAL ANESTHETIC ESTERS\": \"OPHTHALMIC LOCAL ANESTHETICS\",\n",
    "        \"HUMAN INSULINS - SHORT ACTING\": \"INSULIN\",\n",
    "        \"ANTIHYPERGLYCEMIC - MEGLITINIDE ANALOGS\": \"MEGLITINIDE ANALOGUES\",\n",
    "        \"DIRECT ACTING VASODILATORS\": \"VASODILATORS\",\n",
    "        \"LAXATIVE - STIMULANT\": \"STIMULANT LAXATIVES\",\n",
    "        \"SALICYLATE ANALGESICS\": \"SALICYLATES\",\n",
    "        \"DIAGNOSTIC DRUGS - CARDIOVASCULAR\": \"DIAGNOSTIC DRUGS\",\n",
    "    }\n",
    "\n",
    "    missing_mapping = []\n",
    "\n",
    "    for i, reference in enumerate(left):\n",
    "\n",
    "        comparators = []\n",
    "        sorted_results = []\n",
    "        partial_results = []\n",
    "        single_results =   []\n",
    "\n",
    "        if reference in mapping.keys():\n",
    "            print('Already have mapping: ', reference, ' -> ',mapping[reference])\n",
    "            print()\n",
    "            continue\n",
    "        if reference in manual_mapping.keys():\n",
    "            print('Overrided with manual mapping: ', reference,' -> ', manual_mapping[reference])\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "\n",
    "        for comparator in right:\n",
    "            comparators.append(comparator)\n",
    "            sorted_results.append(fuzz.token_sort_ratio(reference,comparator))\n",
    "            partial_results.append(fuzz.partial_ratio(reference,comparator))\n",
    "            single_results.append(fuzz.ratio(reference,comparator))\n",
    "        \n",
    "        sorted_distances = pd.DataFrame({'comparator': comparators, 'similarity': sorted_results}).sort_values(by=['similarity'],ascending=False).reset_index()\n",
    "        partial_distances = pd.DataFrame({'comparator': comparators, 'similarity': partial_results}).sort_values(by=['similarity'],ascending=False).reset_index()\n",
    "        single_distances = pd.DataFrame({'comparator': comparators, 'similarity': single_results}).sort_values(by=['similarity'],ascending=False).reset_index()\n",
    "\n",
    "        if reference in exceptions:\n",
    "            print('Exception')\n",
    "            print('Top candidates: ', '\\n', sorted_distances['comparator'][0], sorted_distances['similarity'][0], '\\n',\n",
    "                  partial_distances['comparator'][0], partial_distances['similarity'][0], '\\n',\n",
    "                  single_distances['comparator'][0], single_distances['similarity'][0])\n",
    "            missing_mapping.append(reference)\n",
    "            continue\n",
    "\n",
    "\n",
    "        print(f'Comparing {reference}')\n",
    "        if sorted_distances['similarity'][0] >= 90:\n",
    "            print('Found similar: ', sorted_distances['comparator'][0], sorted_distances['similarity'][0])\n",
    "            mapping[reference] = sorted_distances['comparator'][0]\n",
    "        elif single_distances['similarity'][0] >= 90:\n",
    "            diff = difflib.ndiff(reference, single_distances['comparator'][0])\n",
    "            \n",
    "            if not any(c.isnumeric() for c in diff):\n",
    "                if not any(c in ['I','N'] for c in diff):\n",
    "                    mapping[reference] = single_distances['comparator'][0]\n",
    "                    print('Found similar: ', single_distances['comparator'][0], single_distances['similarity'][0])\n",
    "            else:\n",
    "                missing_mapping.append(reference)\n",
    "                print('Reject: ', single_distances['comparator'][0], single_distances['similarity'][0])\n",
    "        elif partial_distances['similarity'][0] >= 90:\n",
    "            diff = difflib.ndiff(reference, partial_distances['comparator'][0])\n",
    "\n",
    "            if not any(c.isnumeric() for c in diff):\n",
    "                mapping[reference] = partial_distances['comparator'][0]\n",
    "                print('Found similar: ', partial_distances['comparator'][0], partial_distances['similarity'][0])\n",
    "        else:\n",
    "            print('No similarity')\n",
    "            print('Top candidates: ', '\\n', sorted_distances['comparator'][0], sorted_distances['similarity'][0], '\\n',\n",
    "                  partial_distances['comparator'][0], partial_distances['similarity'][0], '\\n',\n",
    "                  single_distances['comparator'][0], single_distances['similarity'][0])\n",
    "            missing_mapping.append(reference)\n",
    "\n",
    "        print()\n",
    "\n",
    "    print(\"Manually mapped: \", len(manual_mapping))\n",
    "    print(\"Automatically mapped: \", len(mapping))\n",
    "    print(\"Missing: \", len(missing_mapping))\n",
    "\n",
    "    mapping = mapping | manual_mapping\n",
    "    return mapping\n",
    "\n",
    "only_cedars_meds = (set(cedars_meds['PHARM_SUBCLASS'].unique())).difference(set(ucla_meds['PHARM_SUBCLASS'].unique()))\n",
    "all_ucla_meds = set(ucla_meds['PHARM_SUBCLASS'].unique())\n",
    "\n",
    "mapping = create_mapping_dict(only_cedars_meds, all_ucla_meds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Cedars Rows: \", len(cedars_meds))\n",
    "print(\"Total Cedars Rows Remapped: \", len(cedars_meds[cedars_meds['PHARM_SUBCLASS'].isin(mapping.keys())]))\n",
    "\n",
    "print(\"Total UCLA Rows: \", len(ucla_meds))\n",
    "print(\"Total UCLA Rows Corresponding to Mapping: \", len(ucla_meds[ucla_meds['PHARM_SUBCLASS'].isin(mapping.values())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_meds[\"PHARM_SUBCLASS\"] = cedars_meds[\"PHARM_SUBCLASS\"].replace(\n",
    "    mapping\n",
    "    )\n",
    "\n",
    "assess_value_alignment(cedars_meds, ucla_meds, cols=['PHARM_SUBCLASS'], len_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_meds['PHARM_SUBCLASS'].value_counts())\n",
    "print(ucla_meds['PHARM_SUBCLASS'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_labs = data.load.read_files_and_combine(['Labs.txt'], args.cedars_crrt_data_dir)\n",
    "ucla_labs = data.load.read_files_and_combine(['Labs.txt'], args.ucla_crrt_data_dir)\n",
    "\n",
    "cedars_labs = data.longitudinal_features.map_encounter_to_patient(args.cedars_crrt_data_dir, cedars_labs)\n",
    "# Cedars alignment. Assume contact date is diagnosis date since that's the date we have\n",
    "cedars_labs = cedars_labs.rename({\"RESULT\": \"RESULTS\", \n",
    "                              'NAME': 'COMPONENT_NAME'}, axis=1)\n",
    "ucla_labs = ucla_labs.rename({\"RESULT\": \"RESULTS\", \n",
    "                              'NAME': 'COMPONENT_NAME'}, axis=1)\n",
    "\n",
    "assess_value_alignment(cedars_labs, ucla_labs, cols=['COMPONENT_NAME'], len_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucla_labs_temp = ucla_labs.copy()\n",
    "cedars_labs_temp = cedars_labs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = (set(cedars_labs_temp['COMPONENT_NAME'].unique())).difference(set(ucla_labs_temp['COMPONENT_NAME'].unique()))\n",
    "right = set(ucla_labs_temp['COMPONENT_NAME'].unique())\n",
    "distance = jelly_pairwise_diff(left,right)\n",
    "print(distance)\n",
    "single_distance = distance[distance['Distance'] == 1]\n",
    "print(single_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = (set(cedars_labs_temp['COMPONENT_NAME'].unique())).difference(set(ucla_labs_temp['COMPONENT_NAME'].unique()))\n",
    "right = set(ucla_labs_temp['COMPONENT_NAME'].unique())\n",
    "distsort = fuzzy_pairwise_diff(left, right, mode='sort')\n",
    "print(distsort)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notes\n",
    "* as opposed to medications, there are many more labs, so it is harder to manually inspect\n",
    "\n",
    "* It seems like there are single character differences (e.g., hyphen or comma)\n",
    "\n",
    "* There are also cases where the words in a string are the same but reordered\n",
    "\n",
    "* We only use these two rules to match labs\n",
    "\n",
    "* Below, we apply regex to remove the non-alphanumeric characters. Then we look for single character differences that are either an empty space or and 'S' (plurality). We then get the 'sorted' differences that look for the same strings but in different word orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ucla_labs_temp['PSEUDO_COMPONENT_NAME'] = ucla_labs_temp['COMPONENT_NAME'].str.replace('[^a-zA-Z0-9]', ' ', regex=True)\n",
    "ucla_labs_temp['PSEUDO_COMPONENT_NAME'] = ucla_labs_temp['PSEUDO_COMPONENT_NAME'].apply(lambda x: re.sub(' +', ' ', x).strip())\n",
    "\n",
    "cedars_labs_temp['PSEUDO_COMPONENT_NAME'] = cedars_labs_temp['COMPONENT_NAME'].str.replace('[^a-zA-Z0-9]', ' ', regex=True)\n",
    "cedars_labs_temp['PSEUDO_COMPONENT_NAME'] = cedars_labs_temp['PSEUDO_COMPONENT_NAME'].apply(lambda x: re.sub(' +', ' ', x).strip())\n",
    "\n",
    "left = (set(cedars_labs_temp['PSEUDO_COMPONENT_NAME'].unique())).difference(set(ucla_labs_temp['PSEUDO_COMPONENT_NAME'].unique()))\n",
    "right = set(ucla_labs_temp['PSEUDO_COMPONENT_NAME'].unique())\n",
    "distance = jelly_pairwise_diff(left,right)\n",
    "single_distance = distance[distance['Distance'] == 1]\n",
    "\n",
    "cedars_mapping = []\n",
    "ucla_mapping = []\n",
    "for i, row in single_distance.iterrows():\n",
    "\n",
    "    characters_left = dict.fromkeys(row.name[0],0)\n",
    "    characters_right = dict.fromkeys(row.name[1],0)\n",
    "\n",
    "    diff_chars = set(characters_left.keys()).symmetric_difference(set(characters_right.keys()))\n",
    "    if len(diff_chars) > 0:\n",
    "        if 'S' in diff_chars or ' ' in diff_chars:\n",
    "            cedars_mapping.append(row.name[0])\n",
    "            ucla_mapping.append(row.name[1])\n",
    "    else:\n",
    "\n",
    "        for character in characters_left.keys():\n",
    "            characters_left[character] = row.name[0].count(character)\n",
    "\n",
    "            characters_right[character] = row.name[1].count(character)\n",
    "        \n",
    "            if characters_left[character] != characters_right[character]:\n",
    "                if character == 'S' or character == ' ':\n",
    "                    cedars_mapping.append(row.name[0])\n",
    "                    ucla_mapping.append(row.name[1])                                                       \n",
    "print(len(cedars_mapping))\n",
    "\n",
    "\n",
    "left = (set(cedars_labs_temp['PSEUDO_COMPONENT_NAME'].unique())).difference(set(ucla_labs_temp['PSEUDO_COMPONENT_NAME'].unique()))\n",
    "right = set(ucla_labs_temp['PSEUDO_COMPONENT_NAME'].unique())\n",
    "distsort = fuzzy_pairwise_diff(left, right, mode='sort')\n",
    "\n",
    "resorted = distsort[distsort['Distance'] >= 97]\n",
    "for i, row in resorted.iterrows():\n",
    "    if 'FACTOR' not in row.name[1]:\n",
    "        cedars_mapping.append(row.name[0])\n",
    "        ucla_mapping.append(row.name[1])\n",
    "print(len(cedars_mapping))\n",
    "\n",
    "resorted = distsort[distsort['Distance'] == 96]\n",
    "for i, row in resorted.iterrows():\n",
    "    if 'ESTIMATE' in row.name[1]:\n",
    "        cedars_mapping.append(row.name[0])\n",
    "        ucla_mapping.append(row.name[1])\n",
    "\n",
    "print(len(cedars_mapping))\n",
    "same = set(cedars_labs_temp['PSEUDO_COMPONENT_NAME']).intersection(ucla_labs_temp['PSEUDO_COMPONENT_NAME'])\n",
    "same = list(same)\n",
    "\n",
    "cedars_mapping = cedars_mapping + same\n",
    "ucla_mapping = ucla_mapping + same\n",
    "print(len(cedars_mapping))\n",
    "\n",
    "cedars_mapping_df = cedars_labs_temp[cedars_labs_temp['PSEUDO_COMPONENT_NAME'].isin(cedars_mapping)]\n",
    "ucla_mapping_df = ucla_labs_temp[ucla_labs_temp['PSEUDO_COMPONENT_NAME'].isin(ucla_mapping)]\n",
    "\n",
    "cedars_lab_mapping = dict(\n",
    "    zip(cedars_mapping_df[\"COMPONENT_NAME\"], cedars_mapping_df[\"PSEUDO_COMPONENT_NAME\"])\n",
    ")\n",
    "\n",
    "pseudo_lab_mapping = dict(\n",
    "    zip(cedars_mapping, ucla_mapping)\n",
    ")\n",
    "\n",
    "ucla_lab_mapping= dict(\n",
    "    zip(ucla_mapping_df[\"PSEUDO_COMPONENT_NAME\"], ucla_mapping_df[\"COMPONENT_NAME\"])\n",
    ")\n",
    "\n",
    "final_mapping = {}\n",
    "\n",
    "for key in cedars_lab_mapping:\n",
    "    final_mapping[key] = ucla_lab_mapping[pseudo_lab_mapping[cedars_lab_mapping[key]]]\n",
    "print(len(final_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total Cedars Rows: ', len(cedars_labs_temp))\n",
    "print('Total Cedars Rows Remapped: ', len(cedars_labs_temp[cedars_labs_temp['COMPONENT_NAME'].isin(final_mapping.keys())]))\n",
    "\n",
    "print('Total UCLA Rows: ', len(ucla_labs_temp))\n",
    "print('Total UCLA Rows Corresponding to Mapping: ', len(ucla_labs_temp[ucla_labs_temp['COMPONENT_NAME'].isin(final_mapping.values())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cedars_labs_temp[\"COMPONENT_NAME\"] = cedars_labs_temp[\"COMPONENT_NAME\"].replace(\n",
    "    final_mapping\n",
    "    )\n",
    "\n",
    "assess_value_alignment(cedars_labs_temp, ucla_labs_temp, cols=['COMPONENT_NAME'], len_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cedars_meds['PHARM_SUBCLASS'].value_counts())\n",
    "print(ucla_meds['PHARM_SUBCLASS'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.load\n",
    "\n",
    "preprocessed_df = data.load.load_data(args, \"cedars_crrt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import upsetplot\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "def plot_upsetplot(preprocessed_df: pd.DataFrame, cohort: str, colnames: list[str]):\n",
    "    fig = plt.figure(figsize=(10, 30))\n",
    "    indicator_cols = preprocessed_df[colnames].astype(bool)\n",
    "    data = pd.concat([indicator_cols, preprocessed_df[\"recommend_crrt\"]],axis=1).replace({0: \"Do not recommend CRRT\", 1: \"Recommend CRRT\"})\n",
    "    myplot = upsetplot.UpSet(data.set_index(colnames), intersection_plot_elements=0, show_counts=True, show_percentages=True, element_size=50)\n",
    "    myplot.add_stacked_bars(by=\"recommend_crrt\", colors=cm.Set2, elements=10)\n",
    "    myplot.plot()\n",
    "    plt.title(f\"{cohort} Patient Type Breakdown\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_cols = preprocessed_df.columns[preprocessed_df.columns.str.contains(\"RACE_\")].to_list()\n",
    "race_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_groups =  [f\"{name}_pt_indicator\" for name in [\"heart\", \"liver\", \"infection\"]]\n",
    "for columns in [disease_groups, race_cols]:\n",
    "    plot_upsetplot(preprocessed_df, \"CEDARS CRRT\", columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_df[\"recommend_crrt\"].value_counts())\n",
    "print(preprocessed_df[\"recommend_crrt\"].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = preprocessed_df.drop(preprocessed_df.columns[preprocessed_df.columns.str.contains(\"^Unnamed\")], axis=1)\n",
    "def print_missing_info(filter = None):\n",
    "    if filter is not None:\n",
    "        total_notmissing = sum(~tmp_df[filter].isna().any(axis=1))\n",
    "    else:\n",
    "        total_notmissing = sum(~tmp_df.isna().any(axis=1))\n",
    "    print(f\"Number of patients not missing any data: {total_notmissing}, ({total_notmissing/tmp_df.shape[0] * 100}%)\")\n",
    "print(\"All\")\n",
    "print_missing_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tmp_df.drop([\"infection_pt_indicator\", \"liver_pt_indicator\", \"heart_pt_indicator\", \"recommend_crrt\", \"CRRT Year\"], axis=1).isna().mean() * 100).sort_values()\n",
    "\n",
    "amount_missing = tmp_df.drop(\"recommend_crrt\",axis=1).isna().mean().sort_values()\n",
    "amount_missing.name = \"% Missing\"\n",
    "amount_missing.index.name = \"Variables\"\n",
    "(amount_missing*100).to_csv(join(\"C:/Users/jeffe/OneDrive - UCLA IT Services/UCLA/2023_Winter/Rotation/Data/Cedars\", \"variables_and_amount_missing.csv\"))\n",
    "amount_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df[tmp_df.columns[tmp_df.columns.str.contains(\"_na\")]].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzywuzzy\n",
    "jellyfish"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load for Training/Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from os import getcwd\n",
    "from os.path import join\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, join(getcwd(), \"../module_code\"))\n",
    "\n",
    "import data.load\n",
    "import cli_utils \n",
    "\n",
    "sys.argv = [sys.argv[0]]\n",
    "cli_utils.load_cli_args(\"../options.yml\")\n",
    "args = cli_utils.init_cli_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.sklearn_loaders import SklearnCRRTDataModule\n",
    "from exp.static_learning import generate_filters\n",
    "\n",
    "data = SklearnCRRTDataModule.from_argparse_args(args, filters=generate_filters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_ids = None\n",
    "original_columns = None\n",
    "data_transform = None\n",
    "data.setup(\n",
    "    args,\n",
    "    reference_ids=reference_ids,\n",
    "    reference_cols=original_columns,\n",
    "    data_transform=data_transform,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.train[0].shape)\n",
    "# data.val[0].shape\n",
    "print(data.test[0].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crrtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "88ac95e3320af9a902ad5fb0274d19f204ffbd00dc695a7b7dcd9954ec72a217"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
