{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mlflow_path = \"/home/davina/Private/repos/CRRT/mlruns\"\n",
    "data_path = \"/home/davina/Private/crrt-data\"\n",
    "client = MlflowClient(mlflow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    brier_score_loss,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    average_precision_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "METRIC_MAP = {\n",
    "    \"auroc\": lambda gt, pred_probs, decision_thresh: roc_auc_score(gt, pred_probs),\n",
    "    \"ap\": lambda gt, pred_probs, decision_thresh: average_precision_score(\n",
    "        gt, pred_probs\n",
    "    ),\n",
    "    \"brier\": lambda gt, pred_probs, decision_thresh: brier_score_loss(gt, pred_probs),\n",
    "    \"accuracy\": lambda gt, pred_probs, decision_thresh: accuracy_score(\n",
    "        gt, (pred_probs >= decision_thresh).astype(int)\n",
    "    ),\n",
    "    \"f1\": lambda gt, pred_probs, decision_thresh: f1_score(\n",
    "        gt, (pred_probs >= decision_thresh).astype(int)\n",
    "    ),\n",
    "    \"recall\": lambda gt, pred_probs, decision_thresh: recall_score(\n",
    "        gt, (pred_probs >= decision_thresh).astype(int)\n",
    "    ),\n",
    "    \"specificity\": lambda gt, pred_probs, decision_thresh: recall_score(\n",
    "        gt, (pred_probs >= decision_thresh).astype(int), pos_label=0\n",
    "    ),\n",
    "    \"precision\": lambda gt, pred_probs, decision_thresh: precision_score(\n",
    "        gt, (pred_probs >= decision_thresh).astype(int)\n",
    "    ),\n",
    "    # \"conf_matrix\": lambda gt, pred_probs, decision_thresh: confusion_matrix(\n",
    "        # gt, (pred_probs >= decision_thresh).astype(int)\n",
    "    # ),\n",
    "    \"TN\": lambda gt, pred_probs, decision_thresh: confusion_matrix(\n",
    "        gt, (pred_probs >= decision_thresh).astype(int)\n",
    "    )[0, 0],\n",
    "    \"FN\": lambda gt, pred_probs, decision_thresh: confusion_matrix(\n",
    "        gt, (pred_probs >= decision_thresh).astype(int)\n",
    "    )[1, 0],\n",
    "    \"TP\": lambda gt, pred_probs, decision_thresh: confusion_matrix(\n",
    "        gt, (pred_probs >= decision_thresh).astype(int)\n",
    "    )[1, 1],\n",
    "    \"FP\": lambda gt, pred_probs, decision_thresh: confusion_matrix(\n",
    "        gt, (pred_probs >= decision_thresh).astype(int)\n",
    "    )[0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"serialize-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude  // tune trial and //eval best\n",
    "window_runs = client.search_runs(\n",
    "    experiment_ids=client.get_experiment_by_name(\"static_learning\").experiment_id,\n",
    "    filter_string=f'tags.mlflow.runName=\"{experiment_name}\"'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = client.search_runs(\n",
    "    experiment_ids=client.get_experiment_by_name(\"static_learning\").experiment_id,\n",
    "    filter_string=f'tags.mlflow.runName=\"{experiment_name} // eval best\"'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = window_runs + best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [run.info.artifact_uri for run in runs]\n",
    "# [run.data.tags[\"slide_window_by\"] for run in runs]\n",
    "runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "lookback = {\"days\": 5, \"hours\": 18}\n",
    "each_window_results = [\n",
    "    run for run in runs\n",
    "    if (run.data.tags.get(\"slide_window_by\", None) is not None)\n",
    "    and (datetime.now() - datetime.fromtimestamp(run.info.start_time/1000) <= timedelta(**lookback))\n",
    "]\n",
    "[run.data.tags[\"slide_window_by\"] for run in each_window_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle(\"/home/davina/Private/repos/CRRT/predict_probas/xgb_val__predict_probas.pkl\")\n",
    "shapes = []\n",
    "labels = []\n",
    "predict_probas = []\n",
    "slides = []\n",
    "for run in each_window_results:\n",
    "    i = run.data.tags[\"slide_window_by\"]\n",
    "    slides.append(i)\n",
    "    if int(i):\n",
    "        file = f\"df_[startdate+{i}-7d,startdate+{i}].parquet\"\n",
    "    else:\n",
    "        file = \"df_[startdate-7d,startdate].parquet\"\n",
    "    f_df = pd.read_parquet(join(data_path, file))\n",
    "\n",
    "    shapes.append(f_df.shape)\n",
    "    labels.append(f_df[\"recommend_crrt\"])\n",
    "\n",
    "    predict_probas.append(\n",
    "        pd.read_pickle(\n",
    "            join(run.info.artifact_uri, \"predict_probas/lgb_test__predict_probas.pkl\")\n",
    "            # join(run.info.artifact_uri, \"predict_probas/xgb_test__predict_probas.pkl\")\n",
    "            # join(run.info.artifact_uri, \"xgb_test__predict_probas.pkl\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_describe = {}\n",
    "for run in each_window_results:\n",
    "    i = run.data.tags[\"slide_window_by\"]\n",
    "    slides.append(i)\n",
    "    if int(i):\n",
    "        file = f\"df_[startdate+{i}-7d,startdate+{i}].parquet\"\n",
    "    else:\n",
    "        file = \"df_[startdate-7d,startdate].parquet\"\n",
    "    f_df = pd.read_parquet(join(data_path, file))\n",
    "    top_features = [\"CT-INTEM_skew\", \"CT-EXTEM_skew\", \"ABSOLUTE NUCLEATED RBC COUNT_skew\", \"ALPHA-ANGLE-EX_len\"]\n",
    "    # print(f_df.columns[f_df.columns.str.contains(\"ANGLE\")])\n",
    "    # print(f_df[top_features].describe())\n",
    "    features_describe[i] = f_df[top_features].describe()\n",
    "# [df.loc[\"mean\",\"CT-INTEM_skew\"] for df in features_describe]\n",
    "df = pd.concat(features_describe.values(), axis=0, keys=features_describe.keys()).sort_index(level=0)\n",
    "melted = df.melt(ignore_index=False,var_name=\"Features\").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in df.columns:\n",
    "#     print(feature)\n",
    "#     g = sns.FacetGrid(\n",
    "#         melted[melted[\"Features\"] == feature],\n",
    "#         col=\"level_1\", col_wrap=4, sharey=False\n",
    "#     )\n",
    "#     g.map(sns.lineplot, \"level_0\", \"value\")\n",
    "\n",
    "g = sns.FacetGrid( melted, col=\"level_1\", col_wrap=4, sharey=False)\n",
    "g.map(sns.lineplot, \"level_0\", \"value\", hue=melted[\"Features\"])\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "\n",
    "file = join(each_window_results[0].info.artifact_uri, \"img_artifacts\", \"xgb_test__feature_importance.png\")\n",
    "img = mpimg.imread(file)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "smallest_dataset = np.argmin(np.array(shapes)[:,0])\n",
    "reference_ids = predict_probas[smallest_dataset].index\n",
    "\n",
    "metrics = {}\n",
    "for label, predict_proba, slide in zip(labels, predict_probas, slides):\n",
    "    label_idxs = label.index.intersection(reference_ids)\n",
    "    predict_proba_idxs = predict_proba.index.intersection(reference_ids)\n",
    "    metrics[slide] = {\n",
    "        metric_name: metric_fn(label[label_idxs], predict_proba[predict_proba_idxs], 0.5)\n",
    "        for metric_name, metric_fn in METRIC_MAP.items()\n",
    "    }\n",
    "equal_sized_df = pd.DataFrame.from_dict(metrics, orient=\"index\").rename_axis(index=\"Slide\").sort_index(axis=0)\n",
    "equal_sized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.lineplot(data=metrics_over_windows)\n",
    "# g = sns.relplot(data=metrics_over_windows, kind=\"line\")\n",
    "# g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "# plt.show()\n",
    "g = sns.FacetGrid(\n",
    "    equal_sized_df.melt(ignore_index=False,var_name=\"Metrics\").reset_index(),\n",
    "    col=\"Metrics\", col_wrap=4, sharey=False\n",
    ")\n",
    "g.map(sns.lineplot, \"Slide\", \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No filter, visualize metrics across windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_over_windows = pd.DataFrame(\n",
    "    [run.data.metrics for run in each_window_results],\n",
    "    index=[run.data.tags[\"slide_window_by\"] for run in each_window_results]\n",
    ").rename_axis(index=\"Slide\").sort_index(axis=0)\n",
    "# metrics_over_windows.columns = pd.MultiIndex.from_product([[\"Metrics\"], metrics_over_windows.columns])\n",
    "metrics_over_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# g = sns.lineplot(data=metrics_over_windows)\n",
    "# g = sns.relplot(data=metrics_over_windows, kind=\"line\")\n",
    "# g.legend(loc='center left', bbox_to_anchor=(1.25, 0.5), ncol=1)\n",
    "# plt.show()\n",
    "g = sns.FacetGrid(\n",
    "    metrics_over_windows.melt(ignore_index=False,var_name=\"Metrics\").reset_index(),\n",
    "    col=\"Metrics\", col_wrap=4, sharey=False\n",
    ")\n",
    "g.map(sns.lineplot, \"Slide\", \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUROC ONLY\n",
    "auroc_mask = metrics_over_windows.columns.str.contains(\"auroc\")\n",
    "auroc_cols = metrics_over_windows.columns[auroc_mask]\n",
    "g = sns.FacetGrid(\n",
    "    metrics_over_windows[auroc_cols].melt(ignore_index=False,var_name=\"Metrics\").reset_index(),\n",
    "    col=\"Metrics\", col_wrap=4, sharey=False\n",
    ")\n",
    "g.map(sns.lineplot, \"Slide\", \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "# for run in runs:\n",
    "#     client.delete_run(run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('crrt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ec4323cdb57771f81a9fbb6644b264a95b8ef7a3669230ca02ff6d9a0ad522e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
